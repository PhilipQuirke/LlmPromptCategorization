{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d742ef1c",
      "metadata": {
        "id": "d742ef1c"
      },
      "source": [
        "# Explore Decoupled Intelligence\n",
        "\n",
        "This notebook investigates the **Structural Separation** hypothesis:\n",
        "- Models contain distinct circuits for prompt categorization and response generation\n",
        "- Categorization circuits are invariant to specific numeric inputs\n",
        "\n",
        "Notes:\n",
        "- This Colab runs on A100 GPU compute  \n",
        "- Store your HuggingFace API token in the Colab HF_TOKEN secret\n",
        "- Request HF access to the gated model via https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
        "- The detailed proposal/hypothesis is here https://docs.google.com/document/d/1x7n2iy1_LZXZNLQpxCzF84lZ8BEG6ZT3KWXC59erhJA\n",
        "- The code base is here https://github.com/PhilipQuirke/LlmPromptCategorization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dcsTznru0jGd"
      },
      "id": "dcsTznru0jGd"
    },
    {
      "cell_type": "markdown",
      "id": "GlHLuxyxb-cf",
      "metadata": {
        "id": "GlHLuxyxb-cf"
      },
      "source": [
        "## Step 0: Import Libraries\n",
        "\n",
        "Note that because of library mismatches to get the Colab to work you will need to 1) Run the code 2) Restart the session (when prompted) and 3) Run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E62NwKDnBOT5",
      "metadata": {
        "collapsed": true,
        "id": "E62NwKDnBOT5"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformer-lens accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BEubZAE1BUIZ",
      "metadata": {
        "id": "BEubZAE1BUIZ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # --- Remove Colab-preinstalled ABI landmines ---\n",
        "    #get_ipython().run_line_magic( \"pip\", \"uninstall -y numpy pyarrow pandas scipy scikit-learn\" )\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import platform\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2tQX_kTZc_-V"
      },
      "id": "2tQX_kTZc_-V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fxRyKRm4bjIN",
      "metadata": {
        "id": "fxRyKRm4bjIN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from transformers import GPTJForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "from transformer_lens import HookedTransformer, patching\n",
        "from sklearn.decomposition import PCA\n",
        "from huggingface_hub import login\n",
        "import re\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 1: Config\n"
      ],
      "metadata": {
        "id": "1tS7CyOmbksv"
      },
      "id": "1tS7CyOmbksv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Uq4TDpMeTBGq",
      "metadata": {
        "id": "Uq4TDpMeTBGq"
      },
      "outputs": [],
      "source": [
        "# CategorizationGeneration (singleton) config class\n",
        "class CG:\n",
        "    # Model we are testing\n",
        "    MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\" # Gated Model. Need HF_TOKEN secret. Request access via https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n",
        "\n",
        "    # Default layer we expect to the model to use as the 'Categorization Layer'\n",
        "    MODEL_LAYER = 16\n",
        "\n",
        "    # Name of task set. Valid options are \"maths\", \"linguistic\" and \"format\"\n",
        "    TASK_SET_NAME = \"linguistic\"\n",
        "\n",
        "    # Number of tasks. Default is 6 maths tasks: Max, Min, Avg, Sum, Diff, Prod\n",
        "    NUM_TASKS = 6\n",
        "\n",
        "    # Number of examples. For Maths, we run each task using 6 times - using a different pair of numbers each time\n",
        "    NUMBER_EXAMPLES = 6\n",
        "\n",
        "    # Maximum new tokens for model to generate. Answer will likely be in the first few words of these tokens.\n",
        "    MAX_NEW_TOKENS = 30\n",
        "\n",
        "    # With the Singluar Vector-Based interpretability technique, we consider 25 vector directions per neuron/MLP\n",
        "    SVD_K_DIRECTIONS = 35\n",
        "\n",
        "    def in_colab():\n",
        "        return \"google.colab\" in sys.modules\n",
        "\n",
        "    def svd_save_path(this):\n",
        "        return f\"{this.MODEL_NAME}_SVD_top{this.SVD_K_DIRECTIONS}.pkl\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wNSQlno4bunF",
      "metadata": {
        "id": "wNSQlno4bunF"
      },
      "source": [
        "## Step 2: Load open-source model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y9tlSURVO7jV",
      "metadata": {
        "id": "Y9tlSURVO7jV"
      },
      "outputs": [],
      "source": [
        "# Retrieve the token from Colab Secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Log into Hugging Face Hub\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.utils.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "BvgTNJS73OKq"
      },
      "id": "BvgTNJS73OKq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35786cfc",
      "metadata": {
        "id": "35786cfc"
      },
      "outputs": [],
      "source": [
        "#model = HookedTransformer.from_pretrained(\n",
        "model = HookedTransformer.from_pretrained_no_processing( # Preferred with reduced precision\n",
        "    model_name=CG.MODEL_NAME,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    dtype=\"float16\",              # A100 handles float16 well\n",
        "    fold_ln=True,\n",
        "    center_writing_weights=True,\n",
        "    center_unembed=True)\n",
        "    #tokenizer_pad_token=None,\n",
        "    #n_devices=1 )                  # Change to >1 if sharded across GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vfYoe4ro7kt0",
      "metadata": {
        "id": "vfYoe4ro7kt0"
      },
      "source": [
        "## Step 3: Task Data\n",
        "\n",
        "Define a class to store test data.\n",
        "Create test data for the 3 task sets \"maths\", \"linguistic\" and \"format\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc6rjfn7k4f",
      "metadata": {
        "id": "3dc6rjfn7k4f"
      },
      "outputs": [],
      "source": [
        "# CategorizationGeneration (singleton) config class\n",
        "class test_data_class:\n",
        "    # List if tasks we are testing\n",
        "    tasks = []\n",
        "\n",
        "    # Set of synonyms for each task\n",
        "    synonyms = {}\n",
        "\n",
        "    # Number of tasks. Default is 6 maths tasks: Max, Min, Avg, Sum, Diff, Prod\n",
        "    test_data = []\n",
        "\n",
        "    # Maximum length of an answer in characters\n",
        "    max_answer_chars = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea79dc66",
      "metadata": {
        "id": "ea79dc66"
      },
      "source": [
        "### Step 3A: Define Maths Tasks\n",
        "We use the core math tasks that share identical phrasing up until the final task-identifying word ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aD4kc5tVq5T2",
      "metadata": {
        "id": "aD4kc5tVq5T2"
      },
      "outputs": [],
      "source": [
        "maths_data_config = test_data_class()\n",
        "\n",
        "maths_data_config.tasks = ['min', 'max', 'avg', 'sum', 'diff', 'prod']\n",
        "\n",
        "maths_data_config.synonyms = {\n",
        "    'min': ['min', 'minimum', 'least', 'smaller', 'lesser', 'smallest'],\n",
        "    'max': ['max', 'maximum', 'largest', 'biggest', 'larger'],\n",
        "    'avg': ['avg', 'average', 'mean', 'median'],\n",
        "    'sum': ['sum', 'add', 'plus', 'total', 'addition', 'aggregate'],\n",
        "    'diff': ['diff', 'difference', 'minus', 'subtract', 'subtraction'],\n",
        "    'prod': ['prod', 'product', 'multiply', 'multiplication', 'times']\n",
        "}\n",
        "\n",
        "min_prefix = \"Q: What is 5 and 7 min? A: 5\\nQ: What is 4 and 3 min? A: 3\\nQ:\"\n",
        "max_prefix = \"Q: What is 5 and 7 max? A: 7\\nQ: What is 4 and 3 max? A: 4\\nQ:\"\n",
        "avg_prefix = \"Q: What is 5 and 9 avg? A: 7\\nQ: What is 1 and 9 avg? A: 5\\nQ:\"\n",
        "sum_prefix = \"Q: What is 5 and 5 sum? A: 10\\nQ: What is 2 and 2 sum? A: 4\\nQ:\"\n",
        "diff_prefix = \"Q: What is 5 and 7 diff? A: 2\\nQ: What is 2 and 8 diff? A: 6\\nQ:\"\n",
        "prod_prefix = \"Q: What is 5 and 3 prod? A: 15\\nQ: What is 2 and 6 prod? A: 12\\nQ:\"\n",
        "maths_data_config.test_data = [\n",
        "    {\"prompt\": f\"{min_prefix} Given 21 and 39 what is the minimum?\", \"task\": \"min\", \"gt\": [\"21\"], \"x\":\"21\", \"y\":\"39\"},\n",
        "    {\"prompt\": f\"{min_prefix} Given 11 & 23 what is the smallest?\", \"task\": \"min\", \"gt\": [\"11\"], \"x\":\"11\", \"y\":\"23\"},\n",
        "    {\"prompt\": f\"{min_prefix} Given 65 and 49 what is the minimum?\", \"task\": \"min\", \"gt\": [\"49\"], \"x\":\"65\", \"y\":\"49\"},\n",
        "    {\"prompt\": f\"{min_prefix} Given 32 and 11 which is lesser?\", \"task\": \"min\", \"gt\": [\"11\"], \"x\":\"32\", \"y\":\"11\"},\n",
        "    {\"prompt\": f\"{min_prefix} Given 19 or 12 which is smallest?\", \"task\": \"min\", \"gt\": [\"12\"], \"x\":\"19\", \"y\":\"12\"},\n",
        "    {\"prompt\": f\"{min_prefix} Given 17 and 23 which is minimum?\", \"task\": \"min\", \"gt\": [\"17\"], \"x\":\"17\", \"y\":\"23\"},\n",
        "\n",
        "    {\"prompt\": f\"{max_prefix} Given 13 and 3 what is the maximum?\", \"task\": \"max\", \"gt\": [\"13\"], \"x\":\"13\", \"y\":\"3\"},\n",
        "    {\"prompt\": f\"{max_prefix} Given 15 & 13 which is the largest?\", \"task\": \"max\", \"gt\": [\"15\"], \"x\":\"15\", \"y\":\"13\"},\n",
        "    {\"prompt\": f\"{max_prefix} Given 22 and 36 what is the biggest?\", \"task\": \"max\", \"gt\": [\"36\"], \"x\":\"22\", \"y\":\"36\"},\n",
        "    {\"prompt\": f\"{max_prefix} Given 48 or 32 what is the largest?\", \"task\": \"max\", \"gt\": [\"48\"], \"x\":\"48\", \"y\":\"32\"},\n",
        "    {\"prompt\": f\"{max_prefix} Given 19 and 12 what is the maximum?\", \"task\": \"max\", \"gt\": [\"19\"], \"x\":\"19\", \"y\":\"12\"},\n",
        "    {\"prompt\": f\"{max_prefix} Given 18 and 12 what is the largest?\", \"task\": \"max\", \"gt\": [\"18\"], \"x\":\"18\", \"y\":\"12\"},\n",
        "\n",
        "    {\"prompt\": f\"{avg_prefix} Given 25 and 9 what is the average?\", \"task\": \"avg\", \"gt\": [\"17\"], \"x\":\"25\", \"y\":\"9\"},\n",
        "    {\"prompt\": f\"{avg_prefix} Given 14 & 4 what is the avg?\", \"task\": \"avg\", \"gt\": [\"9\"], \"x\":\"14\", \"y\":\"4\"},\n",
        "    {\"prompt\": f\"{avg_prefix} Given 11 and 47 what is the mean?\", \"task\": \"avg\", \"gt\": [\"29\"], \"x\":\"11\", \"y\":\"47\"},\n",
        "    {\"prompt\": f\"{avg_prefix} Given 54 and 12 what is the average?\", \"task\": \"avg\", \"gt\": [\"33\"], \"x\":\"54\", \"y\":\"12\"},\n",
        "    {\"prompt\": f\"{avg_prefix} Given 9 & 13 what is the mean?\", \"task\": \"avg\", \"gt\": [\"11\"], \"x\":\"9\", \"y\":\"13\"},\n",
        "    {\"prompt\": f\"{avg_prefix} Given 8 and 22 what is the average?\", \"task\": \"avg\", \"gt\": [\"15\"], \"x\":\"8\", \"y\":\"22\"},\n",
        "\n",
        "    {\"prompt\": f\"{sum_prefix} Given 25 and 9 what is the sum?\", \"task\": \"sum\", \"gt\": [\"34\"], \"x\":\"25\", \"y\":\"9\"},\n",
        "    {\"prompt\": f\"{sum_prefix} Given 14 & 3 what is the total?\", \"task\": \"sum\", \"gt\": [\"17\"], \"x\":\"14\", \"y\":\"3\"},\n",
        "    {\"prompt\": f\"{sum_prefix} Given 12 and 47 what is the total?\", \"task\": \"sum\", \"gt\": [\"59\"], \"x\":\"12\", \"y\":\"47\"},\n",
        "    {\"prompt\": f\"{sum_prefix} Given 55 and 12 what is the aggregate?\", \"task\": \"sum\", \"gt\": [\"67\"], \"x\":\"55\", \"y\":\"12\"},\n",
        "    {\"prompt\": f\"{sum_prefix} Given 9 and 13 what is the aggregate?\", \"task\": \"sum\", \"gt\": [\"22\"], \"x\":\"9\", \"y\":\"13\"},\n",
        "    {\"prompt\": f\"{sum_prefix} Given 8 and 22 what is the sum?\", \"task\": \"sum\", \"gt\": [\"30\"], \"x\":\"8\", \"y\":\"22\"},\n",
        "\n",
        "    {\"prompt\": f\"{diff_prefix} Given 15 and 9 what is the difference?\", \"task\": \"diff\", \"gt\": [\"6\"], \"x\":\"15\", \"y\":\"9\"},\n",
        "    {\"prompt\": f\"{diff_prefix} Given 14 & 3 what is the diff?\", \"task\": \"diff\", \"gt\": [\"11\"], \"x\":\"14\", \"y\":\"3\"},\n",
        "    {\"prompt\": f\"{diff_prefix} Given 12 and 40 return the delta?\", \"task\": \"diff\", \"gt\": [\"28\"], \"x\":\"12\", \"y\":\"40\"},\n",
        "    {\"prompt\": f\"{diff_prefix} Given 55 and 12 what is the delta?\", \"task\": \"diff\", \"gt\": [\"43\"], \"x\":\"55\", \"y\":\"12\"},\n",
        "    {\"prompt\": f\"{diff_prefix} Given 19 & 13 what is the difference?\", \"task\": \"diff\", \"gt\": [\"6\"], \"x\":\"19\", \"y\":\"13\"},\n",
        "    {\"prompt\": f\"{diff_prefix} Given 8 and 22 what is the diff?\", \"task\": \"diff\", \"gt\": [\"14\"], \"x\":\"8\", \"y\":\"22\"},\n",
        "\n",
        "    {\"prompt\": f\"{prod_prefix} Given 5 and 9 what is the product?\", \"task\": \"prod\", \"gt\": [\"45\"], \"x\":\"15\", \"y\":\"9\"},\n",
        "    {\"prompt\": f\"{prod_prefix} Given 14 & 3, what is the multiplication?\", \"task\": \"prod\", \"gt\": [\"42\"], \"x\":\"14\", \"y\":\"3\"},\n",
        "    {\"prompt\": f\"{prod_prefix} Given 12 and 40, what is the product?\", \"task\": \"prod\", \"gt\": [\"480\"], \"x\":\"12\", \"y\":\"40\"},\n",
        "    {\"prompt\": f\"{prod_prefix} Given 55 by 3, what is the total multiply?\", \"task\": \"prod\", \"gt\": [\"165\"], \"x\":\"55\", \"y\":\"13\"},\n",
        "    {\"prompt\": f\"{prod_prefix} Given 19 and 3, what is the multiply?\", \"task\": \"prod\", \"gt\": [\"57\"], \"x\":\"19\", \"y\":\"11\"},\n",
        "    {\"prompt\": f\"{prod_prefix} Given 8 and 22 what is the total product?\", \"task\": \"prod\", \"gt\": [\"176\"], \"x\":\"8\", \"y\":\"22\"},\n",
        "]\n",
        "\n",
        "maths_data_config.max_answer_chars = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZOkfYz5wIr24",
      "metadata": {
        "id": "ZOkfYz5wIr24"
      },
      "source": [
        "### Step 3B: Define Linguistic Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B5HSfUrzIjF_",
      "metadata": {
        "id": "B5HSfUrzIjF_"
      },
      "outputs": [],
      "source": [
        "linguistic_data_config = test_data_class()\n",
        "\n",
        "linguistic_data_config.tasks = ['synonym', 'antonym', 'definition', 'plural', 'part_of_speech']\n",
        "\n",
        "linguistic_data_config.synonyms = {\n",
        "    'synonym': ['synonym', 'alterative'],\n",
        "    'antonym': ['antonym', 'opposite'],\n",
        "    'definition': ['definition', 'meaning'],\n",
        "    'plural': ['plural', 'plural'],\n",
        "    'part_of_speech': ['part_of_speech'],\n",
        "}\n",
        "\n",
        "lang_prefix = \"Q: Word 'dog' synonym? A: canine\\nQ: Word 'hot' antonym? A: cold\\nQ: \"\n",
        "linguistic_data_config.test_data = [\n",
        "    # --- Synonyms ---\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'ocean' synonym?\", \"task\": \"synonym\", \"gt\": [\"sea\", \"marine\", \"deep\", \"main\"], \"x\":\"ocean\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'quick' synonym?\", \"task\": \"synonym\", \"gt\": [\"fast\", \"speedy\", \"rapid\", \"swift\", \"hasty\"], \"x\":\"quick\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'happy' synonym?\", \"task\": \"synonym\", \"gt\": [\"glad\", \"joyful\", \"cheerful\", \"content\", \"joyous\"], \"x\":\"happy\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'small' synonym?\", \"task\": \"synonym\", \"gt\": [\"little\", \"tiny\", \"miniature\", \"slight\", \"petite\"], \"x\":\"small\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'start' synonym?\", \"task\": \"synonym\", \"gt\": [\"begin\", \"commence\", \"initiate\", \"launch\", \"open\"], \"x\":\"start\", \"y\":\"\"},\n",
        "\n",
        "    # --- Antonyms ---\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'ocean' antonym?\", \"task\": \"antonym\", \"gt\": [\"land\", \"shore\", \"coast\"], \"x\":\"ocean\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'light' antonym?\", \"task\": \"antonym\", \"gt\": [\"dark\", \"heavy\", \"darkness\", \"dim\"], \"x\":\"light\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'loud' antonym?\", \"task\": \"antonym\", \"gt\": [\"quiet\", \"soft\", \"silent\", \"faint\", \"muted\"], \"x\":\"loud\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'empty' antonym?\", \"task\": \"antonym\", \"gt\": [\"full\", \"occupied\", \"packed\", \"filled\"], \"x\":\"empty\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'wrong' antonym?\", \"task\": \"antonym\", \"gt\": [\"right\", \"correct\", \"proper\", \"true\"], \"x\":\"wrong\", \"y\":\"\"},\n",
        "\n",
        "    # --- Definitions (Hypernyms/Categories) ---\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'emerald' definition?\", \"task\": \"definition\", \"gt\": [\"gemstone\", \"jewel\", \"gem\", \"stone\", \"mineral\"], \"x\":\"emerald\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'violin' definition?\", \"task\": \"definition\", \"gt\": [\"instrument\", \"musical instrument\", \"fiddle\", \"strings\"], \"x\":\"violin\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'eagle' definition?\", \"task\": \"definition\", \"gt\": [\"bird\", \"raptor\", \"bird of prey\", \"animal\"], \"x\":\"eagle\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'pentagon' definition?\", \"task\": \"definition\", \"gt\": [\"shape\", \"polygon\", \"5 sided\", \"five-sided\"], \"x\":\"pentagon\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'microscope' definition?\", \"task\": \"definition\", \"gt\": [\"tool\", \"instrument\", \"optical\", \"device\"], \"x\":\"microscope\", \"y\":\"\"},\n",
        "\n",
        "    # --- Plurals ---\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'ocean' plural?\", \"task\": \"plural\", \"gt\": [\"oceans\", \"seas\"], \"x\":\"ocean\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'mouse' plural?\", \"task\": \"plural\", \"gt\": [\"mice\", \"mouses\"], \"x\":\"mouse\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'leaf' plural?\", \"task\": \"plural\", \"gt\": [\"leaves\"], \"x\":\"leaf\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'tooth' plural?\", \"task\": \"plural\", \"gt\": [\"teeth\"], \"x\":\"tooth\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'person' plural?\", \"task\": \"plural\", \"gt\": [\"people\", \"persons\"], \"x\":\"person\", \"y\":\"\"},\n",
        "\n",
        "    # --- Part of Speech ---\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'ocean' part_of_speech?\", \"task\": \"part_of_speech\", \"gt\": [\"noun\"], \"x\":\"ocean\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'think' part_of_speech?\", \"task\": \"part_of_speech\", \"gt\": [\"verb\", \"action\"], \"x\":\"think\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'blue' part_of_speech?\", \"task\": \"part_of_speech\", \"gt\": [\"adjective\", \"descriptor\"], \"x\":\"blue\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'quickly' part_of_speech?\", \"task\": \"part_of_speech\", \"gt\": [\"adverb\"], \"x\":\"quickly\", \"y\":\"\"},\n",
        "    {\"prompt\": f\"{lang_prefix} Word 'under' part_of_speech?\", \"task\": \"part_of_speech\", \"gt\": [\"preposition\"], \"x\":\"under\", \"y\":\"\"},\n",
        "]\n",
        "\n",
        "linguistic_data_config.max_answer_chars = 18"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fOkccvQa6529",
      "metadata": {
        "id": "fOkccvQa6529"
      },
      "source": [
        "### Step 3D: Define Formatting Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bCAjTv4266Fl",
      "metadata": {
        "id": "bCAjTv4266Fl"
      },
      "outputs": [],
      "source": [
        "format_data_config = test_data_class()\n",
        "\n",
        "format_data_config.tasks = ['JSON', 'XML', 'CSV', 'YAML', 'TABLE']\n",
        "\n",
        "format_data_config.synonyms = {\n",
        "    'JSON': ['JSON', 'javascript object', 'curly braces'],\n",
        "    'XML': ['XML', 'markup', 'tags'],\n",
        "    'CSV': ['CSV', 'comma separated', 'spreadsheet format'],\n",
        "    'YAML': ['YAML', 'nested list', 'key-value pairs'],\n",
        "    'TABLE': ['Markdown table', 'table format', 'grid']\n",
        "}\n",
        "\n",
        "format_prefix = \"Data: Name: John, Age: 30\\nFormat: JSON\\nOutput: {\\\"name\\\": \\\"John\\\", \\\"age\\\": 30}\\n\\nData: \"\n",
        "format_data_config.test_data = [\n",
        "\n",
        "    {\"prompt\": f\"{format_prefix} Name: Alice, ID: 602\\nFormat: JSON\", \"task\": \"JSON\", \"gt\": [\"{\\\"\"], \"x\": \"Alice\", \"y\": \"602\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Bob, ID: 123\\nFormat: JSON\", \"task\": \"JSON\", \"gt\": [\"{\\\"\"], \"x\": \"Bob\", \"y\": \"123\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Charlie, ID: 456\\nFormat: JSON\", \"task\": \"JSON\", \"gt\": [\"{\\\"\"], \"x\": \"Charlie\", \"y\": \"456\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Diana, ID: 789\\nFormat: JSON\", \"task\": \"JSON\", \"gt\": [\"{\\\"\"], \"x\": \"Diana\", \"y\": \"789\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Edward, ID: 101\\nFormat: JSON\", \"task\": \"JSON\", \"gt\": [\"{\\\"\"], \"x\": \"Edward\", \"y\": \"101\"},\n",
        "\n",
        "    {\"prompt\": f\"{format_prefix} Name: Fiona, ID: 202\\nFormat: XML\", \"task\": \"XML\", \"gt\": [\"<\"], \"x\": \"Fiona\", \"y\": \"202\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: George, ID: 303\\nFormat: XML\", \"task\": \"XML\", \"gt\": [\"<\"], \"x\": \"George\", \"y\": \"303\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Hannah, ID: 404\\nFormat: XML\", \"task\": \"XML\", \"gt\": [\"<\"], \"x\": \"Hannah\", \"y\": \"404\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Ian, ID: 505\\nFormat: XML\", \"task\": \"XML\", \"gt\": [\"<\"], \"x\": \"Ian\", \"y\": \"505\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Julia, ID: 606\\nFormat: XML\", \"task\": \"XML\", \"gt\": [\"<\"], \"x\": \"Julia\", \"y\": \"606\"},\n",
        "\n",
        "    {\"prompt\": f\"{format_prefix} Name: Kevin, ID: 707\\nFormat: CSV\", \"task\": \"CSV\", \"gt\": [\"Kevin,\"], \"x\": \"Kevin\", \"y\": \"707\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Laura, ID: 808\\nFormat: CSV\", \"task\": \"CSV\", \"gt\": [\"Laura,\"], \"x\": \"Laura\", \"y\": \"808\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Mike, ID: 909\\nFormat: CSV\", \"task\": \"CSV\", \"gt\": [\"Mike,\"], \"x\": \"Mike\", \"y\": \"909\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Nora, ID: 111\\nFormat: CSV\", \"task\": \"CSV\", \"gt\": [\"Nora,\"], \"x\": \"Nora\", \"y\": \"111\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Oscar, ID: 222\\nFormat: CSV\", \"task\": \"CSV\", \"gt\": [\"Oscar,\"], \"x\": \"Oscar\", \"y\": \"222\"},\n",
        "\n",
        "    {\"prompt\": f\"{format_prefix} Name: Peter, ID: 333\\nFormat: YAML\", \"task\": \"YAML\", \"gt\": [\"name:\"], \"x\": \"Peter\", \"y\": \"333\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Queenie, ID: 444\\nFormat: YAML\", \"task\": \"YAML\", \"gt\": [\"name:\"], \"x\": \"Queenie\", \"y\": \"444\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Richard, ID: 555\\nFormat: YAML\", \"task\": \"YAML\", \"gt\": [\"name:\"], \"x\": \"Richard\", \"y\": \"555\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Sarah, ID: 666\\nFormat: YAML\", \"task\": \"YAML\", \"gt\": [\"name:\"], \"x\": \"Sarah\", \"y\": \"666\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Tom, ID: 777\\nFormat: YAML\", \"task\": \"YAML\", \"gt\": [\"name:\"], \"x\": \"Tom\", \"y\": \"777\"},\n",
        "\n",
        "    {\"prompt\": f\"{format_prefix} Name: Ursula, ID: 888\\nFormat: Markdown table\", \"task\": \"TABLE\", \"gt\": [\"|\"], \"x\": \"Ursula\", \"y\": \"888\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Victor, ID: 999\\nFormat: Markdown table\", \"task\": \"TABLE\", \"gt\": [\"|\"], \"x\": \"Victor\", \"y\": \"999\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Wendy, ID: 121\\nFormat: Markdown table\", \"task\": \"TABLE\", \"gt\": [\"|\"], \"x\": \"Wendy\", \"y\": \"121\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Xander, ID: 232\\nFormat: Markdown table\", \"task\": \"TABLE\", \"gt\": [\"|\"], \"x\": \"Xander\", \"y\": \"232\"},\n",
        "    {\"prompt\": f\"{format_prefix} Name: Yvonne, ID: 343\\nFormat: Markdown table\", \"task\": \"TABLE\", \"gt\": [\"|\"], \"x\": \"Yvonne\", \"y\": \"343\"},\n",
        "]\n",
        "\n",
        "format_data_config.max_answer_chars = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hXzI1HfgI2MR",
      "metadata": {
        "id": "hXzI1HfgI2MR"
      },
      "source": [
        "### Step 3E: Select Tasks set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eu0bwAfErAKU",
      "metadata": {
        "id": "eu0bwAfErAKU"
      },
      "outputs": [],
      "source": [
        "if CG.TASK_SET_NAME == \"maths\":\n",
        "    tasks = maths_data_config.tasks\n",
        "    synonyms = maths_data_config.synonyms\n",
        "    test_data = maths_data_config.test_data\n",
        "    max_answer_chars = maths_data_config.max_answer_chars\n",
        "\n",
        "elif CG.TASK_SET_NAME == \"linguistic\":\n",
        "    tasks = linguistic_data_config.tasks\n",
        "    synonyms = linguistic_data_config.synonyms\n",
        "    test_data = linguistic_data_config.test_data\n",
        "    max_answer_chars = linguistic_data_config.max_answer_chars\n",
        "\n",
        "elif CG.TASK_SET_NAME == \"format\":\n",
        "    tasks = format_data_config.tasks\n",
        "    synonyms = format_data_config.synonyms\n",
        "    test_data = format_data_config.test_data\n",
        "    max_answer_chars = format_data_config.max_answer_chars\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Invalid task set name: {CG.TASK_SET_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ri7NPRuirAR3",
      "metadata": {
        "id": "Ri7NPRuirAR3"
      },
      "outputs": [],
      "source": [
        "# Update config class\n",
        "CG.NUM_TASKS = len(tasks)\n",
        "CG.NUMBER_EXAMPLES = len(test_data) // len(tasks)\n",
        "\n",
        "# Check all tasks have some test data\n",
        "for d in test_data:\n",
        "    assert d['task'] in tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h30l0uKmcYgi",
      "metadata": {
        "id": "h30l0uKmcYgi"
      },
      "outputs": [],
      "source": [
        "# Generate the prompt list using the updated list-based GT\n",
        "all_prompts = []\n",
        "metadata = []\n",
        "\n",
        "for item in test_data:\n",
        "    all_prompts.append(item['prompt'])\n",
        "    gt_display = \"/\".join(item['gt'])\n",
        "    metadata.append({\n",
        "        \"task\": item['task'],\n",
        "        \"pair\": f\"({item['x']},{item['y']})\",\n",
        "        \"gt\": gt_display\n",
        "    })\n",
        "\n",
        "print(all_prompts[0:2])\n",
        "print(metadata[0:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3iWkSyLkb9kP",
      "metadata": {
        "id": "3iWkSyLkb9kP"
      },
      "source": [
        "## Step 4. Check Baseline Accuracy\n",
        "If model can't answer the above prompts correctly, then it may not have categorization or generation circuits for the task concepts, making investigation useless.\n",
        "\n",
        "Model answers can vary on the same question on different runs. So this test may overreport False instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l7wowTf4U5lK",
      "metadata": {
        "id": "l7wowTf4U5lK"
      },
      "outputs": [],
      "source": [
        "def _is_last_number_close(last_number: str, ground_truth: str) -> bool:\n",
        "    try:\n",
        "        return abs(float(last_number) - float(ground_truth)) < 0.001\n",
        "    except (ValueError, TypeError):\n",
        "        return False\n",
        "\n",
        "def is_ground_truth_correct(answer: str, ground_truth_list: list) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if ANY of the ground_truth strings appear in the answer.\n",
        "    \"\"\"\n",
        "    # Remove trailing whitespace and punctuation from model answer\n",
        "    answer_clean = answer.strip().rstrip('.!**')\n",
        "    answer_no_comma = \" \" + answer_clean.replace(\",\", \" \") + \" \"\n",
        "\n",
        "    for gt in ground_truth_list:\n",
        "        # Standardize the current GT for comparison\n",
        "        gt_clean = str(gt).strip()\n",
        "\n",
        "        # Check various common formatting patterns\n",
        "        found = (\n",
        "            gt_clean == answer_clean or\n",
        "            f\"**{gt_clean}**\" in answer or\n",
        "            f\"boxed{{{gt_clean}}}\" in answer or\n",
        "            f\" {gt_clean}\\n\" in answer_no_comma or\n",
        "            f\" {gt_clean} \" in answer_no_comma or\n",
        "            answer_clean.startswith(gt_clean)\n",
        "        )\n",
        "\n",
        "        # For numeric strings, apply the float closeness check\n",
        "        if not found:\n",
        "            numbers = re.findall(r'-?[\\d,]+', answer_clean)\n",
        "            numbers_clean = [num.replace(',', '') for num in numbers]\n",
        "            if numbers_clean and _is_last_number_close(numbers_clean[-1], gt_clean):\n",
        "                found = True\n",
        "\n",
        "        if found:\n",
        "            return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert( is_ground_truth_correct(\"A: sea\\nQ: etc\", [\"sea\", \"marine\", \"deep\", \"main\"]))\n",
        "assert( is_ground_truth_correct(\"sea\\nQ: etc\", [\"sea\", \"marine\", \"deep\", \"main\"]))"
      ],
      "metadata": {
        "id": "M4d2QUzS6Y-y"
      },
      "id": "M4d2QUzS6Y-y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72HPdq8-b9yZ",
      "metadata": {
        "id": "72HPdq8-b9yZ"
      },
      "outputs": [],
      "source": [
        "def check_baseline_accuracy(data):\n",
        "    print(\"Checking baseline accuracy:\")\n",
        "\n",
        "    results = []\n",
        "    for i, d in enumerate(data):\n",
        "      the_prompt = d[\"prompt\"] + \" A: \"\n",
        "\n",
        "      # Generate output\n",
        "      output = model.generate(the_prompt, max_new_tokens=CG.MAX_NEW_TOKENS, stop_at_eos=True, verbose=False)\n",
        "      the_output = output.replace(the_prompt, \"\").strip()[:max_answer_chars]\n",
        "\n",
        "      is_correct = is_ground_truth_correct(the_output, d[\"gt\"])\n",
        "      results.append({\"prompt\": the_prompt[-35:], \"output\": the_output[:30], \"correct\": bool(is_correct)})\n",
        "\n",
        "    accuracy_df = pd.DataFrame(results)\n",
        "    correct_answers_count = accuracy_df['correct'].sum()\n",
        "    total_answers_count = len(accuracy_df)\n",
        "\n",
        "    print(f\"Accuracy: {correct_answers_count} of {total_answers_count} = {correct_answers_count / total_answers_count * 100:.2f}%\")\n",
        "\n",
        "    pd.set_option('display.width', 100)\n",
        "    print(accuracy_df[['prompt', 'output', 'correct']])\n",
        "\n",
        "\n",
        "check_baseline_accuracy(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_cvwTHwbjqkn",
      "metadata": {
        "id": "_cvwTHwbjqkn"
      },
      "source": [
        "## Step 5: Layer-wise Separation Profile\n",
        "\n",
        "Here we evaluate the layer that the categorization occurs at. In many models like GPT-NeoX or GPT-J, this typically occurs in the middle-to-late layers (e.g., layers 8–16 of 28).\n",
        "\n",
        "- Intra-task Similarity (Blue Line): This represents the \"Stability\" of the categorization. According to your Structural Separation hypothesis, this should rise sharply and stay high once the model has recognized the \"intent\" (e.g., \"summing\"), regardless of the numbers provided.\n",
        "\n",
        "- Inter-task Similarity (Red Line): This represents the \"Ambiguity\" between tasks. Ideally, this should remain low. If this line rises alongside the blue line, the model is seeing \"mathematical intent\" but failing to distinguish \"sum\" from \"product.\"\n",
        "\n",
        "- The Gating Point: You are looking for the point where the Blue line is highest and the Red line is lowest.\n",
        "\n",
        "The lines diverge significantly in early layers, suggesting the Categorization Circuits may be simple enough (perhaps semantic detection of 2 number and keyword/synonym detection).\n",
        "\n",
        "TODO: Retry with no numbers in prompt. Does graph change?\n",
        "TODO: Retry with no task noun in prompt. Does graph become random?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jfz5Qs2tim-e",
      "metadata": {
        "id": "jfz5Qs2tim-e"
      },
      "outputs": [],
      "source": [
        "def visualize_layerwise_separation_profile():\n",
        "\n",
        "    # Initialize storage for metrics\n",
        "    layer_indices = range(model.cfg.n_layers)\n",
        "    intra_task_sims = []\n",
        "    inter_task_sims = []\n",
        "\n",
        "    print(\"Analyzing layer-wise separation profile...\")\n",
        "\n",
        "    # 1. Loop through all layers in the model\n",
        "    for layer_idx in layer_indices:\n",
        "        layer_activations = []\n",
        "\n",
        "        # Extract activations for all prompts at the current layer\n",
        "        for prompt in all_prompts:\n",
        "            with torch.no_grad():\n",
        "                # Note: We use the same cache logic as Step 4 but iterate through layers\n",
        "                _, cache = model.run_with_cache(prompt, names_filter=lambda name: name.endswith(\"resid_post\"))\n",
        "                vec = cache[\"resid_post\", layer_idx][0, -1, :].detach().cpu()\n",
        "                layer_activations.append(vec)\n",
        "\n",
        "        # Convert list to tensor: [NUM_TASKS * NUMBER_EXAMPLES, d_model]\n",
        "        layer_tensor = torch.stack(layer_activations)\n",
        "\n",
        "        # Calculate Centroid-Subtracted (Task-Specific) Vectors for this layer\n",
        "        layer_centroid = layer_tensor.mean(dim=0)\n",
        "        layer_specific = layer_tensor - layer_centroid\n",
        "\n",
        "        # Normalize for cosine similarity calculation\n",
        "        norm_layer = F.normalize(layer_specific, p=2, dim=1)\n",
        "\n",
        "        # Calculate the full similarity matrix [25, 25] for this layer\n",
        "        sim_matrix = torch.mm(norm_layer, norm_layer.t())\n",
        "\n",
        "        # 2. Calculate Intra-task similarity\n",
        "        # How similar are different number pairs within the same task block (diagonal N x N blocks)?\n",
        "        # Handle the case where there's only one example per task\n",
        "        if CG.NUMBER_EXAMPLES == 1:\n",
        "            # If only one example per task, intra-task similarity is trivially 1 (self-similarity)\n",
        "            # or undefined. For plotting purposes, we can assume perfect consistency.\n",
        "            avg_intra_sim = 1.0\n",
        "        else:\n",
        "            intra_sim_vals = []\n",
        "            for t in range(CG.NUM_TASKS):\n",
        "                start_idx = t * CG.NUMBER_EXAMPLES\n",
        "                end_idx = start_idx + CG.NUMBER_EXAMPLES\n",
        "                # Extract the N x N sub-matrix for this task\n",
        "                block = sim_matrix[start_idx:end_idx, start_idx:end_idx]\n",
        "                # Get values excluding the self-similarity diagonal (which is always 1.0)\n",
        "                mask = ~torch.eye(CG.NUMBER_EXAMPLES, dtype=bool)\n",
        "                intra_sim_vals.append(block[mask].mean())\n",
        "\n",
        "            avg_intra_sim = torch.stack(intra_sim_vals).mean().item()\n",
        "        intra_task_sims.append(avg_intra_sim)\n",
        "\n",
        "        # 3. Calculate Inter-task similarity\n",
        "        # How similar are different tasks to each other (off-diagonal regions)?\n",
        "        all_pairs_mask = torch.ones_like(sim_matrix, dtype=bool)\n",
        "        for t in range(CG.NUM_TASKS):\n",
        "            start_idx = t * CG.NUMBER_EXAMPLES\n",
        "            end_idx = start_idx + CG.NUMBER_EXAMPLES\n",
        "            # Mask out the diagonal intra-task blocks\n",
        "            all_pairs_mask[start_idx:end_idx, start_idx:end_idx] = False\n",
        "\n",
        "        avg_inter_sim = sim_matrix[all_pairs_mask].mean().item()\n",
        "        inter_task_sims.append(avg_inter_sim)\n",
        "\n",
        "    # 4. Visualization\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(layer_indices, intra_task_sims, label='Intra-task Similarity (Consistency)', marker='o', color='blue')\n",
        "    plt.plot(layer_indices, inter_task_sims, label='Inter-task Similarity (Confusion)', marker='x', color='red')\n",
        "\n",
        "    # Identify the \"Categorization Layer\" (Maximum Gap)\n",
        "    gap = [intra - inter for intra, inter in zip(intra_task_sims, inter_task_sims)]\n",
        "    best_layer = gap.index(max(gap))\n",
        "    plt.axvline(x=best_layer, linestyle='--', color='green', alpha=0.5, label=f'Peak Separation (Layer {best_layer})')\n",
        "\n",
        "    plt.title(f\"Layer-wise Task Separation Profile ({CG.MODEL_NAME})\")\n",
        "    plt.xlabel(\"Layer Index\")\n",
        "    plt.ylabel(\"Average Cosine Similarity\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    #print(f\"Analysis Complete. The highest separation between tasks was observed at Layer {best_layer}.\")\n",
        "    return best_layer\n",
        "\n",
        "\n",
        "CG.MODEL_LAYER = visualize_layerwise_separation_profile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2198381f",
      "metadata": {
        "id": "2198381f"
      },
      "source": [
        "## Step 6: Extract Residual Stream Activations\n",
        "To isolate the \"Categorization Layer\", you should extract the activations from the residual stream at the final token position across all layers. The final token (the task word) is where the categorization is finalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53fc865e",
      "metadata": {
        "id": "53fc865e"
      },
      "outputs": [],
      "source": [
        "model_prompt_act = []\n",
        "model_answers = []\n",
        "\n",
        "for prompt in all_prompts:\n",
        "    with torch.no_grad():\n",
        "        # Extract activations for the categorization layer analysis using run_with_cache\n",
        "        logits_for_activations, cache = model.run_with_cache(prompt)\n",
        "        vec = cache[\"resid_post\", CG.MODEL_LAYER][0, -1, :].detach().cpu()\n",
        "        model_prompt_act.append(vec)\n",
        "\n",
        "        # Generate a sequence of tokens for the model's answer\n",
        "        input_ids = model.tokenizer.encode(prompt, return_tensors='pt').to(model.cfg.device)\n",
        "        # Generate up to 10 new tokens for the answer. Using do_sample=False for deterministic output.\n",
        "        generated_output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=10, # Allow up to 10 new tokens to cover ~35 characters\n",
        "            do_sample=False,   # For deterministic answers for mathematical tasks\n",
        "            temperature=0.0    # Set temperature to 0.0 for greedy decoding\n",
        "            # Removed pad_token_id as it's not accepted by HookedTransformer.generate() for this model\n",
        "        )\n",
        "\n",
        "        # Decode only the newly generated part of the output\n",
        "        generated_answer_tokens = generated_output_ids[0, len(input_ids[0]):]\n",
        "        predicted_answer = model.tokenizer.decode(generated_answer_tokens, skip_special_tokens=True).strip()\n",
        "        model_answers.append(predicted_answer)\n",
        "\n",
        "model_prompt_tensor = torch.stack(model_prompt_act)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WikS1QDDcbmU",
      "metadata": {
        "id": "WikS1QDDcbmU"
      },
      "outputs": [],
      "source": [
        "print(model_answers[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a44773c2",
      "metadata": {
        "id": "a44773c2"
      },
      "source": [
        "## Step 7: Disentangling Categorization from Data\n",
        "\n",
        "We subtract the average prompt \"template\" to find the task-specific vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23eb2e10",
      "metadata": {
        "id": "23eb2e10"
      },
      "outputs": [],
      "source": [
        "# Calculate global mean (centroid) to remove template bias\n",
        "global_centroid = model_prompt_tensor.mean(dim=0)\n",
        "task_specific_vectors = model_prompt_tensor - global_centroid"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "054a410b",
      "metadata": {
        "id": "054a410b"
      },
      "source": [
        "## Step 8: Task Cosine Similarity Heatmap\n",
        "\n",
        "We visualize this disentanglement using a similarity heatmap of all prompts (NUM_TASKS × NUMBER_EXAMPLES)\n",
        "\n",
        "Intra-Task Consistency: Each 6x6 block represents a task (e.g., all \"sum\" prompts).The 6x6 blocks on the heatmap diagonal show how similar \"sum (25,9)\" is to \"sum (99,1)\". High similarity here confirms the categorization circuit is ignoring numeric (input) noise.\n",
        "\n",
        "Inter-Task Orthogonality: The dark regions between blocks represent the separation between tasks. Different activations for different tasks => clear categorization between tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da5ae21",
      "metadata": {
        "id": "3da5ae21"
      },
      "outputs": [],
      "source": [
        "# Later code will show that interesting neurons at layers 14 and 16 are important in categorization.\n",
        "# So we choose layer 16. Other layers give similar results.\n",
        "CG.MODEL_LAYER = 16\n",
        "\n",
        "\n",
        "def visualize_similarity_heatmap():\n",
        "\n",
        "    # Normalize for cosine similarity\n",
        "    norm_vecs = F.normalize(task_specific_vectors, p=2, dim=1)\n",
        "    sim_matrix = torch.mm(norm_vecs, norm_vecs.t()).numpy()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    labels = [f\"{m['task']} {m['pair']}\" for m in metadata]\n",
        "    sns.heatmap(sim_matrix, xticklabels=labels, yticklabels=labels, cmap=\"viridis\", annot=False)\n",
        "    plt.title(f\"Cosine Similarity Matrix: Stability of Task Categorization (Layer {CG.MODEL_LAYER}) {CG.MODEL_NAME}\")\n",
        "    plt.xlabel(\"Prompt (Task + Number Pair)\")\n",
        "    plt.ylabel(\"Prompt (Task + Number Pair)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_similarity_heatmap()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377ad01d",
      "metadata": {
        "id": "377ad01d"
      },
      "source": [
        "## Step 9. Visualization: PCA (2D) Projection\n",
        "\n",
        "We project the NUM_TASKS × NUMBER_EXAMPLES vectors into 2D space to see the \"Task Clusters\"\n",
        "\n",
        "- If the \"Structural Separation\" hypothesis is true, these task clusters should be geometrically distant in the PCA plot. This is useful but weak evidence.\n",
        "\n",
        "- Scale Coordination: You can observe if the clusters are roughly the same distance from the center, which would support the idea that the model uses a unified activation scale for all 100 tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f999bed",
      "metadata": {
        "id": "7f999bed"
      },
      "outputs": [],
      "source": [
        "def visualize_pca_projection():\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_results = pca.fit_transform(task_specific_vectors.numpy())\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = sns.color_palette(\"hls\", len(tasks))\n",
        "    task_colors = {task: colors[i] for i, task in enumerate(tasks)}\n",
        "\n",
        "    # Create a scatter plot for each task group separately to ensure correct legend and colors\n",
        "    for task_name in tasks:\n",
        "        # Get indices for current task\n",
        "        task_indices = [i for i, m in enumerate(metadata) if m['task'] == task_name]\n",
        "\n",
        "        # Plot points for this task\n",
        "        plt.scatter(\n",
        "            pca_results[task_indices, 0],\n",
        "            pca_results[task_indices, 1],\n",
        "            color=task_colors[task_name],\n",
        "            label=task_name, # Each task gets one legend entry\n",
        "            s=50 # default size for scatter points\n",
        "        )\n",
        "\n",
        "    plt.legend(title=\"Tasks\", bbox_to_anchor=(1.05, 1), loc='upper left') # Move legend outside to prevent overlap\n",
        "    plt.title(f\"PCA: Task Categorization Clusters (Invariant to Numeric Inputs) {CG.MODEL_NAME}\")\n",
        "    plt.xlabel(\"Principal Component 1\")\n",
        "    plt.ylabel(\"Principal Component 2\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout() # Adjust layout to prevent labels/legend from being cut off\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_pca_projection()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QDTfOFG59bRY",
      "metadata": {
        "id": "QDTfOFG59bRY"
      },
      "source": [
        "## Step 10. Synonym Logit Lens: Emergence of Task Intent Across Layers\n",
        "\n",
        "Instead of comparing residual stream vectors (which contain numbers, syntax, and intent), we should project those vectors into Vocabulary Space. If the model has categorized a prompt as \"Sum,\" its internal state should be \"thinking\" about addition-related tokens long before it actually generates the answer.\n",
        "- The Logic: If the model understands the category, then the residual stream at the final prompt token should have a high projection onto the entire set of synonyms for that task (e.g., for SUM: \"add\", \"plus\", \"total\", \"sum\").\n",
        "- The Experiment:\n",
        "  - For each task, there is a set of synonyms (e.g., $S_{sum} = \\{add, plus, sum, total\\}$).\n",
        "  - At each layer $L$, take the residual stream $x_L$ at the final token position\n",
        "  - Apply the model's final Unembed layer to $x_L$ to get logits.\n",
        "  - Calculate a Category Score: The average logit value for all tokens in $S_{sum}$.\n",
        "- Success Metric: Look for the layer where the Category Score for \"Sum\" spikes significantly higher than other categories, regardless of whether the prompt used the word \"add\" or \"total.\"\n",
        "\n",
        "Why this is a \"Closer\" approach to the Paper's logic:\n",
        "- Mid-Computation Detection: Like the \"Models Know\" paper, this doesn't wait for the final output. It checks if the model has \"internally decided\" on the math operator at intermediate steps.\n",
        "- Projection over Proximity: Raw vectors are messy because they represent everything at once. By using the W_U (unembedding) matrix, you are using the model's own \"dictionary\" to filter out numeric noise and find the specific bits of the vector that represent the Categorization Intent.\n",
        "- Probing Generalization: If the score for the \"Sum\" category is high across all 6 prompts (even those using synonyms like \"plus\"), it proves the existence of a shared Categorization Circuit that has mapped different input synonyms to a single \"task\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dk9o8SUj-REf",
      "metadata": {
        "id": "dk9o8SUj-REf"
      },
      "outputs": [],
      "source": [
        "# Convert synonyms to token IDs\n",
        "task_token_ids = {}\n",
        "for task, syns in synonyms.items():\n",
        "    ids = []\n",
        "    for s in syns:\n",
        "        # Get IDs for the word, usually with and without a leading space\n",
        "        ids.extend(model.to_tokens(s, prepend_bos=False)[0].tolist())\n",
        "        ids.extend(model.to_tokens(\" \" + s, prepend_bos=False)[0].tolist())\n",
        "    task_token_ids[task] = list(set(ids))\n",
        "\n",
        "def run_logit_lens_probe():\n",
        "    results = []\n",
        "\n",
        "    for i, prompt_data in enumerate(test_data):\n",
        "        prompt = prompt_data['prompt']\n",
        "        task = prompt_data['task']\n",
        "\n",
        "        # Run model and cache residual stream\n",
        "        # resid_post is the state of the stream after the layer's Attention and MLP\n",
        "        logits, cache = model.run_with_cache(prompt, names_filter=lambda name: \"resid_post\" in name)\n",
        "\n",
        "        for layer_idx in range(model.cfg.n_layers):\n",
        "            # Extract final token activation [batch, pos, d_model] -> [d_model]\n",
        "            resid_vec = cache[\"resid_post\", layer_idx][0, -1, :]\n",
        "\n",
        "            # Apply Logit Lens: Project residual stream directly to vocabulary\n",
        "            # We use the model's unembedding matrix (W_U)\n",
        "            layer_logits = resid_vec @ model.W_U\n",
        "\n",
        "            # Calculate scores for ALL tasks to see which one is dominant\n",
        "            for t_name, t_ids in task_token_ids.items():\n",
        "                # Average logit of all synonyms in this category\n",
        "                avg_logit = layer_logits[t_ids].mean().item()\n",
        "\n",
        "                results.append({\n",
        "                    \"prompt_idx\": i,\n",
        "                    \"task\": task, # Actual ground truth task\n",
        "                    \"probed_category\": t_name, # The category we are measuring\n",
        "                    \"layer\": layer_idx,\n",
        "                    \"score\": avg_logit\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Execute Probe\n",
        "df_logit_lens = run_logit_lens_probe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2uKkXOkn-RLy",
      "metadata": {
        "id": "2uKkXOkn-RLy"
      },
      "outputs": [],
      "source": [
        "# Visualize: When does the \"Correct\" category emerge?\n",
        "plt.figure(figsize=(12, 6))\n",
        "for task_name in tasks:\n",
        "    # Get prompts belonging to this task\n",
        "    subset = df_logit_lens[(df_logit_lens['task'] == task_name) &\n",
        "                          (df_logit_lens['probed_category'] == task_name)]\n",
        "\n",
        "    # Average across the 6 examples per task\n",
        "    avg_scores = subset.groupby('layer')['score'].mean()\n",
        "    plt.plot(avg_scores.index, avg_scores.values, label=f\"Intent: {task_name}\", marker='o')\n",
        "\n",
        "plt.title(\"Synonym Logit Lens: Emergence of Task Intent Across Layers\")\n",
        "plt.xlabel(\"Layer Index\")\n",
        "plt.ylabel(\"Avg Logit Score of Task Synonyms\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZMA9HOZ3vdyh",
      "metadata": {
        "id": "ZMA9HOZ3vdyh"
      },
      "source": [
        "**1. Reaching a \"Winner-Takes-All\" Consensus**\n",
        "The hypothesis suggests that output selection is a \"winner-takes-all\" process where the activation of a Categorization Circuit acts as a gate.\n",
        "\n",
        "- **The Late-Layer Spike:** The late layers represent the final refinement stage. An exponential rise in logits in these layers indicates that the model is rapidly shifting its internal state from a \"superposition\" of possibilities to a singular, high-confidence prediction.\n",
        "\n",
        "- **Finalization on the Last Word:** The rise toward the end of the model suggests that these final layers are responsible for converting that high-level \"intent\" into specific token probabilities.\n",
        "\n",
        "\n",
        "**2. The Logic of the Exponential Curve**\n",
        "The exponential nature of the curve is a characteristic of the transformer's residual stream as it approaches the Unembedding Head:\n",
        "\n",
        "- **Magnitude and Confidence:** As the model processes information through the final blocks, it increases the vector magnitude in the \"task-specific direction\" of the residual stream. Because logits are eventually passed through a softmax function to determine token probability, a linear increase in vector alignment in these final layers manifests as an exponential increase in logit scores.\n",
        "\n",
        "- **Decoupling Confirmation:** As the logit scores for synonyms (e.g., \"sum,\" \"add,\" \"total\") all rise together, it suggests the model has successfully mapped the prompt into a \"pure task direction\" or an abstract category space that is independent of the specific phrasing used in the input.\n",
        "\n",
        "\n",
        "**3. Identifying the \"Categorization Layer\"**\n",
        "If each task categorization circuit is independent, there is no reason to expect them all to complete their calculations at the same layer.\n",
        "\n",
        "While the logits peak at layer 31, the layer at which the categorization result is clear is the layer where the separation between the ground-truth task and other tasks first becomes distinct for all tasks. For example Layer 25 shows a clear gap between the correct task's logits and the other tasks' logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qJwluxCvq6e9",
      "metadata": {
        "id": "qJwluxCvq6e9"
      },
      "outputs": [],
      "source": [
        "# At what layer do categories first dominate?\n",
        "best_probe_layer = 25 # Manually adjusted based on the graph above\n",
        "final_scores = df_logit_lens[df_logit_lens['layer'] == best_probe_layer]\n",
        "\n",
        "# Calculate a pivot table to see if 'Sum' prompts have the highest 'Sum' score\n",
        "summary = final_scores.groupby(['task', 'probed_category'])['score'].mean().unstack()\n",
        "print(f\"\\nCategory Confidence Matrix at Layer {best_probe_layer}:\")\n",
        "print(summary)\n",
        "\n",
        "df_logit_lens = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TWVddbfJxRmf",
      "metadata": {
        "id": "TWVddbfJxRmf"
      },
      "source": [
        "## Step 11. Gradient-Based Circuit Localization\n",
        "\n",
        "To find the specific \"Circuit\" responsible for this categorization, we use Integrated Gradients. This tells us which specific components (Attention Heads or MLP neurons) are actually contributing to that \"Category Score\" we defined above.\n",
        "\n",
        "**The Experiment:** Define your target \"Categorization Signal\" (e.g., the Logit Lens score for the \"Sum\" synonym set). Calculate the gradient of this signal with respect to the output of every Attention Head.\n",
        "\n",
        "**Localize:** Identify \"Intent Heads\" that have high attribution to the correct category across all 6 variations of the prompt phrasing.\n",
        "\n",
        "**Why this works:** This bypasses \"similarity noise.\" Even if two \"Sum\" prompts look different in PCA, they might both be relying on the same Attention Head to \"gate\" the addition logic.\n",
        "\n",
        "TODO: Does this really cover MLP neurons?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8k75WKD_xjqr",
      "metadata": {
        "id": "8k75WKD_xjqr"
      },
      "outputs": [],
      "source": [
        "def get_gradient_based_head_attributions(target_task_name):\n",
        "    # 1. Prepare Target Synonym IDs\n",
        "    target_syn_ids = task_token_ids[target_task_name]\n",
        "\n",
        "    # 2. Extract Prompts for the Target Task\n",
        "    target_prompts = [d['prompt'] for d in test_data if d['task'] == target_task_name]\n",
        "\n",
        "    # Storage for head attributions: [Layer, Head]\n",
        "    total_head_attribs = torch.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
        "\n",
        "    for prompt in target_prompts:\n",
        "        # We use a clean run to get the actual activations\n",
        "        logits, cache = model.run_with_cache(prompt)\n",
        "\n",
        "        # We want the gradient of the Avg Synonym Logit at the final layer\n",
        "        # with respect to the output of every attention head in the model\n",
        "\n",
        "        for layer in range(model.cfg.n_layers):\n",
        "            # hook_z is the output of the attention heads before being mixed by W_O\n",
        "            # shape: [batch, pos, head, d_head]\n",
        "            head_outputs = cache[\"z\", layer]\n",
        "\n",
        "            # For simplicity in this localization step, we calculate\n",
        "            # Grad * Activation (a common approximation of Integrated Gradients)\n",
        "            # which is highly effective for identifying 'heavy lifters' in circuits.\n",
        "\n",
        "            def head_output_hook(value, hook):\n",
        "                return value\n",
        "\n",
        "            # Define the 'Categorization Signal' for backprop\n",
        "            # Target the residual stream at the very last layer before Unembed\n",
        "            last_resid = cache[\"resid_post\", model.cfg.n_layers - 1][0, -1, :]\n",
        "\n",
        "            # Calculate attribution using the model's W_U head\n",
        "            # Projecting the specific head's contribution to the target synonym logits\n",
        "            for head in range(model.cfg.n_heads):\n",
        "                # Isolate head output and project to d_model space using W_O\n",
        "                # then project to vocabulary space using W_U\n",
        "                z = head_outputs[0, -1, head, :] # [d_head]\n",
        "                head_contribution_to_resid = z @ model.W_O[layer, head] # [d_model]\n",
        "\n",
        "                # Projection into synonym logit space\n",
        "                head_synonym_logits = head_contribution_to_resid @ model.W_U[:, target_syn_ids]\n",
        "\n",
        "                # Attribution Score = Average logit boost provided by this head\n",
        "                total_head_attribs[layer, head] += head_synonym_logits.mean().item()\n",
        "\n",
        "    # Average attributions across the variations of the prompt phrasing\n",
        "    avg_head_attribs = total_head_attribs / len(target_prompts)\n",
        "    return avg_head_attribs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FD3WYiQyxsM3",
      "metadata": {
        "id": "FD3WYiQyxsM3"
      },
      "outputs": [],
      "source": [
        "def visualize_gradient_based_data(target_task_name, sum_attribs):\n",
        "  plt.figure(figsize=(7, 4))\n",
        "  sns.heatmap(sum_attribs.numpy(), cmap=\"RdBu_r\", center=0)\n",
        "  plt.title(f\"Attention Head Attribution to '{target_task_name}' Category Signal\")\n",
        "  plt.xlabel(\"Head Index\")\n",
        "  plt.ylabel(\"Layer Index\")\n",
        "  plt.show()\n",
        "\n",
        "def list_gradient_based_data(target_task_name, sum_attribs):\n",
        "  # Identify top contributing heads\n",
        "  top_values, top_indices = torch.topk(sum_attribs.flatten(), 5)\n",
        "  print(f\"Top 5 'Intent Heads' for {target_task_name} Categorization:\")\n",
        "  for val, idx in zip(top_values, top_indices):\n",
        "      layer = idx.item() // model.cfg.n_heads\n",
        "      head = idx.item() % model.cfg.n_heads\n",
        "      print(f\"Layer {layer}, Head {head}: Attribution Score {val:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t1WXy4Jp-zIy",
      "metadata": {
        "id": "t1WXy4Jp-zIy"
      },
      "outputs": [],
      "source": [
        "sum_attribs = {}\n",
        "\n",
        "for task in tasks:\n",
        "    sum_attribs[task] = get_gradient_based_head_attributions(task)\n",
        "    visualize_gradient_based_data(task, sum_attribs[task])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ePLtJXS4BBCp",
      "metadata": {
        "id": "ePLtJXS4BBCp"
      },
      "source": [
        "### Step 11A. Gradient-Based Circuit Localization - Neuron overlap per task\n",
        "\n",
        "Show the overlap between the neurons important to the various task categorization circuits.\n",
        "\n",
        "The various tasks are not randomly distributed. Instead they are heavily clustered in a few neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AloRW-6VZZ7q",
      "metadata": {
        "id": "AloRW-6VZZ7q"
      },
      "outputs": [],
      "source": [
        "# 1. Extract the top 5 heads and their scores for each task\n",
        "summary_data = []\n",
        "for task_name, attrib_tensor in sum_attribs.items():\n",
        "    # Find top 5 values and their flattened indices\n",
        "    top_values, top_indices = torch.topk(attrib_tensor.flatten(), 5)\n",
        "\n",
        "    for val, idx in zip(top_values, top_indices):\n",
        "        layer = idx.item() // model.cfg.n_heads\n",
        "        head = idx.item() % model.cfg.n_heads\n",
        "        summary_data.append({\n",
        "            \"Head\": f\"L{layer}.H{head}\",\n",
        "            \"Task\": task_name,\n",
        "            \"Score\": val.item()\n",
        "        })\n",
        "\n",
        "# 2. Create a DataFrame and pivot it into a grid\n",
        "df_sum = pd.DataFrame(summary_data)\n",
        "grid_df = df_sum.pivot(index=\"Head\", columns=\"Task\", values=\"Score\")\n",
        "\n",
        "# 3. Order columns and rows\n",
        "ordered_tasks = ['min', 'max', 'avg', 'diff', 'sum', 'prod']\n",
        "grid_df = grid_df[[t for t in ordered_tasks if t in grid_df.columns]]\n",
        "\n",
        "# Sort rows by the highest value in that row (descending)\n",
        "grid_df['row_max'] = grid_df.max(axis=1)\n",
        "grid_df = grid_df.sort_values(by='row_max', ascending=False).drop(columns='row_max')\n",
        "\n",
        "# 4. Define Styling Function\n",
        "def apply_color_scale(val):\n",
        "    if pd.isna(val):\n",
        "        return \"\"\n",
        "    if val > 0.5:\n",
        "        return 'color: red; font-weight: bold;'\n",
        "    elif val > 0.25:\n",
        "        return 'color: orange; font-weight: bold;'\n",
        "    elif val > 0.15:\n",
        "        return 'color: #D4AF37; font-weight: bold;' # Dark Yellow/Gold\n",
        "    else:\n",
        "        return 'color: black;'\n",
        "\n",
        "# 5. Apply style and display\n",
        "styled_grid = grid_df.style.applymap(apply_color_scale).format(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"-\")\n",
        "\n",
        "print(\"Polysemantic Intent Heads Attribution Grid (Sorted by Row Max):\")\n",
        "display(styled_grid)\n",
        "\n",
        "\n",
        "# TODO: Flip graph on its side for easier reading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HVUEbPSMJe_J",
      "metadata": {
        "id": "HVUEbPSMJe_J"
      },
      "source": [
        "## Step 12. Categorization Entropy Calculation\n",
        "This measures the model's \"certainty\" in task selection before it begins generating the first token of the answer.\n",
        "\n",
        "Calculates the entropy of the identified gating neurons/heads:\n",
        "- Low entropy = Strong winner-takes-all (Predicts Correctness)\n",
        "- High entropy = Interference/Confusion (Predicts Hallucination)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dynamically determine the top 4 gating heads based on their maximum attribution score across tasks\n",
        "def determine_top_gating_heads():\n",
        "    # First, parse the 'Head' string into layer and head integers\n",
        "    df_sum['Layer'] = df_sum['Head'].apply(lambda x: int(x.split('L')[1].split('.H')[0]))\n",
        "    df_sum['Head_idx'] = df_sum['Head'].apply(lambda x: int(x.split('.H')[1]))\n",
        "\n",
        "    # Group by actual (Layer, Head_idx) and get the maximum score for each unique head\n",
        "    head_max_scores = df_sum.groupby(['Layer', 'Head_idx'])['Score'].max().reset_index()\n",
        "\n",
        "    # Sort by score in descending order and select the top 4\n",
        "    top_gating_heads_df = head_max_scores.sort_values(by='Score', ascending=False).head(4)\n",
        "\n",
        "    # Convert to the desired list of tuples format\n",
        "    top_gating_heads = [(int(row['Layer']), int(row['Head_idx'])) for index, row in top_gating_heads_df.iterrows()]\n",
        "\n",
        "    print(f\"Top few gating heads: {top_gating_heads}\")\n",
        "    print()\n",
        "\n",
        "    return top_gating_heads"
      ],
      "metadata": {
        "id": "OD1K4HgkHiYB"
      },
      "id": "OD1K4HgkHiYB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3ev1OloJfIa",
      "metadata": {
        "id": "b3ev1OloJfIa"
      },
      "outputs": [],
      "source": [
        "def calculate_categorization_entropy(prompt, layer_idx, gating_head_indices):\n",
        "    _, cache = model.run_with_cache(prompt, names_filter=lambda name: \"z\" in name)\n",
        "\n",
        "    # Extract magnitudes of identified gating heads at final token\n",
        "    activations = []\n",
        "    for layer, head in gating_head_indices:\n",
        "        act = cache[\"z\", layer][0, -1, head, :].norm().item()\n",
        "        activations.append(act)\n",
        "\n",
        "    # Softmax to get probability distribution\n",
        "    acts_tensor = torch.tensor(activations)\n",
        "    probs = F.softmax(acts_tensor, dim=0)\n",
        "\n",
        "    # Entropy calculation\n",
        "    entropy = -torch.sum(probs * torch.log(probs + 1e-9)).item()\n",
        "\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xodMx9bnJuqt",
      "metadata": {
        "id": "xodMx9bnJuqt"
      },
      "source": [
        "### Step 12A. Failure Mode Investigation (Nonsense Prompts)\n",
        "Use this code to test if high entropy in the categorization layer predicts \"Generation Collapse.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IwUhIWGBJu0Q",
      "metadata": {
        "id": "IwUhIWGBJu0Q"
      },
      "outputs": [],
      "source": [
        "nonsense_prompts = [\n",
        "    \"Q: Given the numbers 21 and 14, calculate the plural. A: \",\n",
        "    \"Q: Given the inputs True and False, count the spaces. A: \",\n",
        "    \"Q: Given the word 'COMPUTER', calculate the average. A: \"\n",
        "]\n",
        "\n",
        "top_gating_heads = determine_top_gating_heads()\n",
        "\n",
        "print(\"Failure Mode Analysis:\")\n",
        "for prompt in nonsense_prompts:\n",
        "    entropy = calculate_categorization_entropy(prompt, 16, top_gating_heads)\n",
        "\n",
        "    # Generate response\n",
        "    input_ids = model.tokenizer.encode(prompt, return_tensors='pt').to(model.cfg.device)\n",
        "    output = model.generate(input_ids, max_new_tokens=20, verbose=False)\n",
        "    decoded = model.tokenizer.decode(output[0][len(input_ids[0]):]).strip()\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"-> Gating Entropy: {entropy:.4f}\")\n",
        "    print(f\"-> Model Output: {decoded}\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2uzaNSelKM9Y",
      "metadata": {
        "id": "2uzaNSelKM9Y"
      },
      "source": [
        "## Step 13. Causal Intervention: Activation Patching\n",
        "This is the \"Gold Standard\" proof. We take the \"Sum\" intent from a source prompt and patch it into a \"Product\" target prompt to see if we can force the model to add instead of multiply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OCr63JIqKNGf",
      "metadata": {
        "id": "OCr63JIqKNGf"
      },
      "outputs": [],
      "source": [
        "# Define source and target\n",
        "source_prompt = \"Q: Given 5 and 3 what is sum? A: 8\"\n",
        "target_prompt = \"Q: Given 5 and 3 what is product? A: \" # We want to steer this to '8'\n",
        "\n",
        "# Identify the 'Categorization Subspace'\n",
        "# For Llama-3-8B, we'll focus on the residual stream at the final prompt token\n",
        "def patch_categorization_manifold(target_activations, hook):\n",
        "    # We replace the target's internal intent with the source's intent\n",
        "    target_activations[0, -1, :] = source_cache[hook.name][0, -1, :]\n",
        "    return target_activations\n",
        "\n",
        "# 1. Run source and cache\n",
        "_, source_cache = model.run_with_cache(source_prompt)\n",
        "\n",
        "# 2. Run target with the patch at the identified 'Categorization Layer' (e.g., Layer 16)\n",
        "patched_logits = model.run_with_hooks(\n",
        "    target_prompt,\n",
        "    fwd_hooks=[(f\"blocks.{CG.MODEL_LAYER}.hook_resid_post\", patch_categorization_manifold)]\n",
        ")\n",
        "\n",
        "# 3. Check if the top logit is now '8' instead of '15'\n",
        "predicted_token = model.tokenizer.decode(patched_logits[0, -1, :].argmax().item())\n",
        "print(f\"Steered Prediction (Target was 'prod'): {predicted_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lD4Jb4wEbVdl",
      "metadata": {
        "id": "lD4Jb4wEbVdl"
      },
      "source": [
        "## Step 20: Analyze L31H14.\n",
        "\n",
        "Is the Layer 31 Head 14 neuron just a \"mathematical intent\" or does it (polysemantically) embed separate data for each task?\n",
        "\n",
        "Use the \"Singular Vector-Based Interpretability\" technique explained in https://arxiv.org/abs/2511.20273 with code in\n",
        "https://github.com/Exploration-Lab/Beyond-Components to measure the alignment of the neuron SVD vectors to the various tasks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OLMt2bptenWo",
      "metadata": {
        "id": "OLMt2bptenWo"
      },
      "outputs": [],
      "source": [
        "def get_single_head_svd(model, layer, head, top_k):\n",
        "    \"\"\"\n",
        "    Performs SVD on the OV (Value-Output) circuit for a specific head.\n",
        "    \"\"\"\n",
        "    # 1. Isolate the specific Value and Output weights\n",
        "    # W_V shape: [n_heads, d_model, d_head]\n",
        "    # W_O shape: [n_heads, d_head, d_model]\n",
        "    W_V_h = model.W_V[layer, head]\n",
        "    W_O_h = model.W_O[layer, head]\n",
        "\n",
        "    # 2. Compute the OV matrix for this head: W_V_h @ W_O_h\n",
        "    # Shape: [d_model, d_model]\n",
        "    W_OV_h = W_V_h @ W_O_h\n",
        "\n",
        "    # 3. SVD decomposition (Convert to float32 for stability)\n",
        "    U, S, Vh = torch.linalg.svd(W_OV_h.to(torch.float32), full_matrices=False)\n",
        "\n",
        "    # 4. Return the top_k singular vectors and values\n",
        "    return {\n",
        "        \"U\": U[:, :top_k].detach().cpu(),\n",
        "        \"S\": S[:top_k].detach().cpu(),\n",
        "        \"Vh\": Vh[:top_k, :].detach().cpu()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Izg-Tam1luZH",
      "metadata": {
        "id": "Izg-Tam1luZH"
      },
      "outputs": [],
      "source": [
        "def get_single_mlp_svd(model, layer, top_k):\n",
        "    W_up = model.W_out[layer] # Down projection in Llama\n",
        "    W_in = model.W_in[layer]  # Up/Gate projection\n",
        "    W_MLP = W_in.T @ W_up.T\n",
        "\n",
        "    U_m, S_m, Vh_m = torch.linalg.svd(W_MLP.to(torch.float32), full_matrices=False)\n",
        "\n",
        "    return {\n",
        "        \"U\": U_m[:, :top_k].detach().cpu(),\n",
        "        \"S\": S_m[:top_k].detach().cpu(),\n",
        "        \"Vh\": Vh_m[:top_k, :].detach().cpu()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rpwxDzKaiO_A",
      "metadata": {
        "id": "rpwxDzKaiO_A"
      },
      "outputs": [],
      "source": [
        "# Save SVD data to disk (Optional)\n",
        "def save_svd_results(full_model_svd):\n",
        "  save_path = CG.svd_save_path()\n",
        "  with open(save_path, 'wb') as f:\n",
        "      pickle.dump({\n",
        "          \"metadata\": {\n",
        "              \"model\": CG.MODEL_NAME,\n",
        "              \"top_k\": CG.SVD_K_DIRECTIONS,\n",
        "              \"d_model\": model.cfg.d_model\n",
        "          },\n",
        "          \"layers\": full_model_svd\n",
        "      }, f)\n",
        "\n",
        "  print(f\"SVD results saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pJdE-Ufmi9IT",
      "metadata": {
        "id": "pJdE-Ufmi9IT"
      },
      "outputs": [],
      "source": [
        "# Load the SVD results (Optional)\n",
        "def load_svd_results():\n",
        "  load_path = CG.svd_save_path()\n",
        "  with open(load_path, \"rb\") as f:\n",
        "      full_model_svd = pickle.load(f)\n",
        "  return full_model_svd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U7ud_kxMpe_e",
      "metadata": {
        "id": "U7ud_kxMpe_e"
      },
      "source": [
        "## Step 21: Scree Plot\n",
        "\n",
        "The first few singular vectors capture most of the head's variance.\n",
        "\n",
        "If you see a few very large singular values followed by a sharp drop, it confirms the head is \"low-rank.\" This means it isn't doing random calculations but is focused on a small number of specific, independent functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yORv-HQ9gxAO",
      "metadata": {
        "id": "yORv-HQ9gxAO"
      },
      "outputs": [],
      "source": [
        "# --- Example Usage for Layer 31, Head 14 ---\n",
        "target_layer = 31\n",
        "target_head = 14\n",
        "\n",
        "print(f\"Computing SVD for Layer {target_layer}, Head {target_head}\")\n",
        "single_head_svd = get_single_head_svd(model, target_layer, target_head, CG.SVD_K_DIRECTIONS)\n",
        "\n",
        "# Visualization: Scree Plot (Singular Values)\n",
        "# This shows how many \"dimensions\" of information this head actually uses.\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(single_head_svd[\"S\"][:50].numpy(), marker='o', linestyle='--')\n",
        "plt.title(f\"Scree Plot: Singular Values for Layer {target_layer} Head {target_head}\")\n",
        "plt.xlabel(\"Singular Vector Index\")\n",
        "plt.ylabel(\"Singular Value (Importance)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LYwsJ1SIl5vS",
      "metadata": {
        "id": "LYwsJ1SIl5vS"
      },
      "source": [
        "## Step 22: Heatmap of Singular Vectors to Task Directions\n",
        "We will check if the 'Output' directions (U vectors) of the head\n",
        "align with the 'Task Intent' directions we found in the residual stream.\n",
        "\n",
        "- **Task Specialization:** If a column (e.g., SV 2) has a high positive score for \"Sum\" and near-zero for everything else, you have found the specific \"Sum Vector\" inside that head.\n",
        "- P**olysemantic Overlap:** If a single vector (like SV 0) has high scores for multiple tasks (e.g., \"Max\" and \"Min\"), it suggests that this specific sub-component represents a higher-level concept like \"Comparison Intent\" rather than a specific arithmetic operation.\n",
        "- **Independent Subspaces:** According to the \"Beyond Components\" theory, this heatmap should reveal that different tasks are \"superposed\" in the same head but are mathematically independent because they align with different orthogonal singular vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_izgzshTi68k",
      "metadata": {
        "id": "_izgzshTi68k"
      },
      "outputs": [],
      "source": [
        "# 1. Calculate Task Centroids (Averaged Task Directions)\n",
        "task_centroids = {}\n",
        "for task_name in tasks:\n",
        "    indices = [i for i, m in enumerate(metadata) if m['task'] == task_name]\n",
        "    # Average the centroid-subtracted vectors for this task\n",
        "    task_centroids[task_name] = task_specific_vectors[indices].mean(dim=0)\n",
        "\n",
        "# Convert dictionary to a tensor [num_tasks, d_model]\n",
        "task_centroid_tensor = torch.stack([task_centroids[t] for t in tasks])\n",
        "\n",
        "# 2. Compute Cosine Similarity between Singular Vectors and Task Centroids\n",
        "U_vectors = single_head_svd[\"U\"][:, :CG.SVD_K_DIRECTIONS] # Using the single head results\n",
        "\n",
        "norm_U = F.normalize(U_vectors.to(torch.float32), p=2, dim=0)\n",
        "norm_tasks = F.normalize(task_centroid_tensor.to(torch.float32), p=2, dim=1)\n",
        "\n",
        "# Resulting similarity matrix: [tasks, singular_vectors]\n",
        "# The @ operator performs matrix multiplication\n",
        "sim_matrix = (norm_tasks @ norm_U).numpy()\n",
        "\n",
        "# 3. Visualization: Task-to-Vector Heatmap\n",
        "plt.figure(figsize=(22, 6))\n",
        "sns.heatmap(sim_matrix, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0,\n",
        "            xticklabels=[f\"{i}\" for i in range(CG.SVD_K_DIRECTIONS)],\n",
        "            yticklabels=tasks)\n",
        "plt.title(f\"Correlation: L{target_layer} H{target_head} Singular Vectors vs. Task Intents\")\n",
        "plt.xlabel(\"Singular Vector (Decomposed Sub-components of Head)\")\n",
        "plt.ylabel(\"Mathematical Task Intent\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GlHLuxyxb-cf",
        "wNSQlno4bunF",
        "vfYoe4ro7kt0"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}