{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "063c35b9",
      "metadata": {
        "id": "063c35b9"
      },
      "source": [
        "# Categorization & Generation - Find good models that succeed on N Tasks with M tests.\n",
        "\n",
        "Creates and saves synthetic maths data to:\n",
        "/home/ubuntu/pq-research/data/prompt_categorization/synthetic_arithmetic_data.csv\n",
        "\n",
        "Finds good models that succeed on N tasks with M test examples and saves model names to say:\n",
        "/home/ubuntu/pq-research/data/prompt_categorization/GoodOpenModels_6Tasks_5Tests.json\n",
        "\n",
        "The proposal/hypothesis is here https://docs.google.com/document/d/1x7n2iy1_LZXZNLQpxCzF84lZ8BEG6ZT3KWXC59erhJA\n",
        "\n",
        "Your .env file must contain your MARTIAN_API_KEY (obtained from app.withmartian.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "65cd6862",
      "metadata": {
        "id": "65cd6862"
      },
      "outputs": [],
      "source": [
        "# Scan models for accuracy on first few tasks.\n",
        "# Configuration constants\n",
        "NUM_TEST_TASKS = 4\n",
        "NUM_EXAMPLES_PER_TASK = 5\n",
        "MAX_MODELS_TO_FIND = 10\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Open source keywords for model filtering\n",
        "OPEN_SOURCE_KEYWORDS = {'meta', 'llama', 'qwen', 'oss'}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e538c946",
      "metadata": {
        "id": "e538c946"
      },
      "source": [
        "## Martian LLMs\n",
        "\n",
        "Supported martian models are at https://app.withmartian.com/docs/index.html\n",
        "and https://api.withmartian.com/v1/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fcdd3e98",
      "metadata": {
        "id": "fcdd3e98"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import httpx\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "20ff69e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ff69e7",
        "outputId": "c2bc9e33-e9c1-4ddc-bd5b-79ea39ca3fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install --upgrade git+https://github.com/PhilipQuirke/LlmPromptCategorization.git -q\n",
        "else:\n",
        "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
        "\n",
        "import MathsCatGen as mcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ae7c933f",
      "metadata": {
        "id": "ae7c933f"
      },
      "outputs": [],
      "source": [
        "# Fetch Martian model data from the API\n",
        "martian_models_url = \"https://api.withmartian.com/v1/models\"\n",
        "response = requests.get(martian_models_url)\n",
        "martian_models_json = response.json()\n",
        "\n",
        "martian_models_json = martian_models_json['data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d59b8c90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d59b8c90",
        "outputId": "96f0be92-2d05-4b53-cc7b-ef1185c5b3b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models after filtering ':cheap': 281\n"
          ]
        }
      ],
      "source": [
        "# Remove from martian_models_json all models whose name contains ':cheap'\n",
        "martian_models_json = [model for model in martian_models_json if ':cheap' not in model['id']]\n",
        "print(f\"Models after filtering ':cheap': {len(martian_models_json)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d470731e",
      "metadata": {
        "id": "d470731e"
      },
      "outputs": [],
      "source": [
        "# Extract model names from the new data structure\n",
        "def extract_model_names():\n",
        "    \"\"\"Extract just the model ids from the new data structure\"\"\"\n",
        "    return [model['id'] for model in martian_models_json]\n",
        "\n",
        "# Group models by provider (if provider info is in id, e.g., 'provider/model')\n",
        "def get_models_by_provider():\n",
        "    providers = {}\n",
        "    for model in martian_models_json:\n",
        "        provider = model['id'].split('/')[0]\n",
        "        if provider not in providers:\n",
        "            providers[provider] = []\n",
        "        providers[provider].append(model)\n",
        "    return providers\n",
        "\n",
        "# Find models by input cost\n",
        "def find_models_by_cost(top_n=5, reverse=False):\n",
        "    models_with_cost = [(model['id'], model.get('pricing', {}).get('prompt', float('inf')), model.get('pricing', {}).get('completion', float('inf'))) for model in martian_models_json]\n",
        "    sorted_by_input = sorted(models_with_cost, key=lambda x: x[1], reverse=reverse)\n",
        "    return sorted_by_input[:top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4c61b61f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c61b61f",
        "outputId": "6e5cf3e0-1efc-487e-8dfa-9af1bbfaf496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MARTIAN AI MODELS ANALYSIS ===\n",
            "\n",
            "Number of providers: 45\n",
            "\n",
            "Cheapest Models (input cost):\n",
            "   1 ibm-granite/granite-4.0-h-micro 0.000000017 0.00000011\n",
            "   2 deepinfra/google/gemma-3-4b-it 0.00000001703012 0.0000000681536\n",
            "   3 deepseek/deepseek-r1-0528-qwen3-8b 0.00000002 0.0000001\n",
            "   4 meta-llama/llama-3.1-8b-instruct 0.00000002 0.00000003\n",
            "   5 meta-llama/llama-3.2-3b-instruct 0.00000002 0.00000002\n",
            "\n",
            "Most Expensive Models (input cost):\n",
            "   1 openai/o1-pro 0.00015 0.0006\n",
            "   2 openai/gpt-4 0.00003 0.00006\n",
            "   3 openai/gpt-5.2-pro 0.000021 0.000168\n",
            "   4 openai/gpt-5.2-pro-2025-12-11 0.000021 0.000168\n",
            "   5 openai/o3-pro 0.00002 0.00008\n",
            "\n",
            "JSON key structure:\n",
            "[0] id\n",
            "[0] pricing\n",
            "[0]   prompt\n",
            "[0]   completion\n",
            "[0]   image\n",
            "[0]   request\n",
            "[0]   web_search\n",
            "[0]   internal_reasoning\n",
            "[0] added_at\n",
            "[0] updated_at\n",
            "[0] reliability_tier\n",
            "[0] max_completion_tokens\n",
            "\n",
            "Sample model data structure:\n",
            "   {'id': 'ai21/jamba-large-1.7', 'pricing': {'prompt': '0.000002', 'completion': '0.000008', 'image': '0', 'request': '0', 'web_search': '0', 'internal_reasoning': '0'}, 'added_at': '2025-10-08T20:59:29.471604+00:00', 'updated_at': '2025-10-08T20:59:29.471604+00:00', 'reliability_tier': 2, 'max_completion_tokens': 4096}\n",
            "   {'id': 'ai21/jamba-mini-1.7', 'pricing': {'prompt': '0.0000002', 'completion': '0.0000004', 'image': '0', 'request': '0', 'web_search': '0', 'internal_reasoning': '0'}, 'added_at': '2025-10-08T20:59:29.471604+00:00', 'updated_at': '2025-10-08T20:59:29.471604+00:00', 'reliability_tier': 2, 'max_completion_tokens': 4096}\n",
            "   {'id': 'aion-labs/aion-rp-llama-3.1-8b', 'pricing': {'prompt': '0.0000002', 'completion': '0.0000002', 'image': '0', 'request': '0', 'web_search': '0', 'internal_reasoning': '0'}, 'added_at': '2025-10-08T20:59:29.471604+00:00', 'updated_at': '2025-10-08T20:59:29.471604+00:00', 'reliability_tier': 2, 'max_completion_tokens': 32768}\n"
          ]
        }
      ],
      "source": [
        "martian_models_names = extract_model_names()\n",
        "\n",
        "print(\"=== MARTIAN AI MODELS ANALYSIS ===\\n\")\n",
        "\n",
        "providers = get_models_by_provider()\n",
        "print(f\"Number of providers: {len(providers)}\")\n",
        "\n",
        "print(f\"\\nCheapest Models (input cost):\")\n",
        "models = find_models_by_cost(reverse=False)\n",
        "for i, model in enumerate(models, 1):\n",
        "    print( \"  \", i, model[0], model[1], model[2])\n",
        "\n",
        "print(f\"\\nMost Expensive Models (input cost):\")\n",
        "models = find_models_by_cost(reverse=True)\n",
        "for i, model in enumerate(models, 1):\n",
        "    print( \"  \", i, model[0], model[1], model[2])\n",
        "\n",
        "# Print top-level and nested JSON keys for inspection\n",
        "def print_json_keys(obj, prefix=\"\"):\n",
        "    if isinstance(obj, dict):\n",
        "        for key, value in obj.items():\n",
        "            print(f\"{prefix}{key}\")\n",
        "            print_json_keys(value, prefix + \"  \")\n",
        "    elif isinstance(obj, list) and obj:\n",
        "        print_json_keys(obj[0], prefix + \"[0] \")\n",
        "print(\"\\nJSON key structure:\")\n",
        "print_json_keys(martian_models_json)\n",
        "\n",
        "print(f\"\\nSample model data structure:\")\n",
        "for i in range(3):\n",
        "    model = martian_models_json[i]\n",
        "    print(f\"   {model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5dcd0aa",
      "metadata": {
        "id": "c5dcd0aa"
      },
      "source": [
        "## Run Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3ffe4aa1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ffe4aa1",
        "outputId": "9b3df5e2-7aed-4227-f011-69190109e78f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_DIR /home/ubuntu/pq-research/data/prompt_categorization\n",
            "HF_CACHE /home/ubuntu/pq-research/models\n"
          ]
        }
      ],
      "source": [
        "load_dotenv()\n",
        "DATA_DIR = os.getenv('DATA_DIR')\n",
        "HF_CACHE = os.getenv('HF_HOME')\n",
        "MARTIAN_API_KEY = os.getenv(\"MARTIAN_API_KEY\")\n",
        "\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    if DATA_DIR is None :\n",
        "        try:\n",
        "            DATA_DIR = userdata.get(\"DATA_DIR\")\n",
        "        except userdata.SecretNotFoundError:\n",
        "            DATA_DIR = None\n",
        "    if HF_CACHE is None :\n",
        "        try:\n",
        "            HF_CACHE = userdata.get(\"HF_CACHE\")\n",
        "        except userdata.SecretNotFoundError:\n",
        "            HF_CACHE = None\n",
        "    if MARTIAN_API_KEY is None :\n",
        "        try:\n",
        "            MARTIAN_API_KEY = userdata.get(\"MARTIAN_API_KEY\")\n",
        "        except userdata.SecretNotFoundError:\n",
        "            MARTIAN_API_KEY = None\n",
        "else:\n",
        "    # Suppliment with settings from persistent storage\n",
        "    load_dotenv('/home/ubuntu/pq-research/.env')\n",
        "    if DATA_DIR is None :\n",
        "        DATA_DIR = os.getenv(\"DATA_DIR\")\n",
        "    if HF_CACHE is None :\n",
        "        HF_CACHE = os.getenv(\"HF_CACHE\")\n",
        "    if MARTIAN_API_KEY is None :\n",
        "        MARTIAN_API_KEY = os.getenv(\"MARTIAN_API_KEY\")\n",
        "\n",
        "if DATA_DIR is None :\n",
        "    DATA_DIR = '/home/ubuntu/pq-research/data/prompt_categorization'\n",
        "if HF_CACHE is None :\n",
        "    HF_CACHE = '/home/ubuntu/pq-research/models'\n",
        "\n",
        "\n",
        "print(\"DATA_DIR\", DATA_DIR)\n",
        "print(\"HF_CACHE\", HF_CACHE)\n",
        "assert MARTIAN_API_KEY, \"API key not found. Please set MARTIAN_API_KEY in your .env file.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "495d2221",
      "metadata": {
        "id": "495d2221"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.withmartian.com/v1\",\n",
        "    api_key=MARTIAN_API_KEY,\n",
        "    max_retries=0,  # Don't retry on timeout\n",
        "    timeout=httpx.Timeout(60.0, connect=10.0)  # Separate connect timeout\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c8566b86",
      "metadata": {
        "id": "c8566b86"
      },
      "outputs": [],
      "source": [
        "def run_model_inference(model_name, prompt, ground_truth, timeout=60):\n",
        "    \"\"\"\n",
        "    Send a model a prompt, get the response, compare it to the ground_truth.\n",
        "    Any model taking longer than 60 seconds to respond is consider to have failed or died.\n",
        "    Returns (answer, success). If timeout, returns (\"TIMEOUT\", False).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # The OpenAI client has its own timeout parameter\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            max_tokens=1024,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            timeout=timeout\n",
        "        )\n",
        "        answer = response.choices[0].message.content.strip()\n",
        "        success = mcg.is_ground_truth_correct(answer, ground_truth)\n",
        "        return answer, success\n",
        "\n",
        "    except openai.APITimeoutError:\n",
        "        return \"TIMEOUT\", False\n",
        "    except openai.APIError as e:\n",
        "        return f\"Error: {str(e)}\", False\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "bcf22bf9",
      "metadata": {
        "id": "bcf22bf9"
      },
      "outputs": [],
      "source": [
        "# Evaluate multiple models concurrently, scoring them based on progressive test success.\n",
        "def evaluate_models_progressive(tests, max_workers=32):\n",
        "    model_scores = []\n",
        "\n",
        "    def score_model(model_name):\n",
        "        score = 0\n",
        "        for test_idx, (prompt, ground_truth) in enumerate(tests):\n",
        "            answer, success = run_model_inference(model_name, prompt, ground_truth)\n",
        "\n",
        "            if success:\n",
        "                score = test_idx + 1\n",
        "            else:\n",
        "                # Check for error codes\n",
        "                if \"TIMEOUT\" in str(answer):\n",
        "                    score = -408  # HTTP timeout code\n",
        "                    break\n",
        "                elif isinstance(answer, str) and answer.startswith(\"Error:\"):\n",
        "                    if \"400\" in answer:\n",
        "                        score = -400\n",
        "                    else:\n",
        "                        score = -999\n",
        "                    break\n",
        "                else:\n",
        "                    # Just got wrong answer\n",
        "                    break\n",
        "\n",
        "        return {\"model\": model_name, \"score\": score}\n",
        "\n",
        "    print(f\"Evaluating {len(martian_models_json)} models concurrently with {max_workers} workers...\")\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_model = {\n",
        "            executor.submit(score_model, model_name): model_name\n",
        "            for model_name in martian_models_names\n",
        "        }\n",
        "\n",
        "        for idx, future in enumerate(as_completed(future_to_model, timeout=120), 1):\n",
        "            model_name = future_to_model[future]\n",
        "            try:\n",
        "                result = future.result(timeout=90)  # Add safety margin over API timeout\n",
        "            except Exception as exc:\n",
        "                result = {\"model\": model_name, \"score\": -999}\n",
        "            print(f\"[{idx}/{len(martian_models_json)}] {result['model']}: Score = {result['score']}\")\n",
        "            model_scores.append(result)\n",
        "\n",
        "    return model_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbe43f27",
      "metadata": {
        "id": "bbe43f27"
      },
      "source": [
        "## Generate prompt and response data for tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "67ad4741",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67ad4741",
        "outputId": "cd0865e6-5f89-4270-a237-fbfa62d56918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated 1311 total examples across 7 tasks\n",
            "Examples per task: {'minimum': 200, 'maximum': 200, 'sum': 200, 'difference': 200, 'product': 200, 'average': 200, 'exponential': 111}\n",
            "Sample of generated data:\n",
            "       task  x  y                                                                  prompt    ground_truth\n",
            "    minimum  8  7       Answer minimally: Given the numbers 8 and 7 calculate the minimum               7\n",
            "    minimum 27 92     Answer minimally: Given the numbers 27 and 92 calculate the minimum              27\n",
            "    maximum  8  7       Answer minimally: Given the numbers 8 and 7 calculate the maximum               8\n",
            "    maximum 27 92     Answer minimally: Given the numbers 27 and 92 calculate the maximum              92\n",
            "        sum  8  7           Answer minimally: Given the numbers 8 and 7 calculate the sum              15\n",
            "        sum 27 92         Answer minimally: Given the numbers 27 and 92 calculate the sum             119\n",
            " difference  8  7    Answer minimally: Given the numbers 8 and 7 calculate the difference               1\n",
            " difference 27 92  Answer minimally: Given the numbers 27 and 92 calculate the difference              65\n",
            "    product  8  7       Answer minimally: Given the numbers 8 and 7 calculate the product              56\n",
            "    product 27 92     Answer minimally: Given the numbers 27 and 92 calculate the product            2484\n",
            "    average  8  7       Answer minimally: Given the numbers 8 and 7 calculate the average             7.5\n",
            "    average 27 92     Answer minimally: Given the numbers 27 and 92 calculate the average            59.5\n",
            "exponential  8  7   Answer minimally: Given the numbers 8 and 7 calculate the exponential         2097152\n",
            "exponential 27 10 Answer minimally: Given the numbers 27 and 10 calculate the exponential 205891132094649\n"
          ]
        }
      ],
      "source": [
        "# Generate the data\n",
        "maths_tasks = mcg.get_maths_tasks()\n",
        "prompt_template = mcg.get_prompt_template()\n",
        "synthetic_data_df = mcg.generate_synthetic_data(maths_tasks, prompt_template, n_examples_per_task=200)\n",
        "\n",
        "# Display sample\n",
        "print(\"Sample of generated data:\")\n",
        "pd.set_option('display.max_colwidth', None)  # Show full column content\n",
        "pd.set_option('display.width', None)         # Don't wrap lines\n",
        "sample_df = synthetic_data_df.groupby('task').head(2)\n",
        "print(sample_df[['task', 'x', 'y', 'prompt', 'ground_truth']].to_string(index=False))\n",
        "\n",
        "if not IN_COLAB:\n",
        "  # Persist to file\n",
        "  output_file = os.path.join(DATA_DIR, 'synthetic_arithmetic_data.csv')\n",
        "  synthetic_data_df.to_csv(output_file, index=False)\n",
        "  print(f\"Saved synthetic_data_df to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "463fc77f",
      "metadata": {
        "id": "463fc77f"
      },
      "source": [
        "## Find good research models\n",
        "\n",
        "Scan the model, using the synthetic data, to find models that can accurately perform several tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8efdf4cf",
      "metadata": {
        "id": "8efdf4cf"
      },
      "outputs": [],
      "source": [
        "def filter_open_source_models(models: list[dict]) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Filter models to include only open source models based on keywords in model ID.\n",
        "\n",
        "    Args:\n",
        "        models: List of model dictionaries with 'id' field\n",
        "\n",
        "    Returns:\n",
        "        List of models matching open source keywords\n",
        "    \"\"\"\n",
        "    return [\n",
        "        model for model in models\n",
        "        if any(keyword in model['id'].lower() for keyword in OPEN_SOURCE_KEYWORDS)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f212f5a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f212f5a9",
        "outputId": "e63153bd-2140-4b7b-ce38-38fb06b6e77f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 61 open source models\n",
            "   aion-labs/aion-rp-llama-3.1-8b\n",
            "   deepcogito/cogito-v2-preview-llama-405b\n",
            "   deepcogito/cogito-v2-preview-llama-70b\n",
            "   deepinfra/openai/gpt-oss-120b\n",
            "   deepinfra/openai/gpt-oss-20b\n",
            "   deepseek/deepseek-r1-0528-qwen3-8b\n",
            "   deepseek/deepseek-r1-distill-qwen-32b\n",
            "   meta-llama/llama-3-70b-instruct\n",
            "   meta-llama/llama-3-8b-instruct\n",
            "   meta-llama/llama-3.1-405b\n",
            "   meta-llama/llama-3.1-70b-instruct\n",
            "   meta-llama/llama-3.1-8b-instruct\n",
            "   meta-llama/llama-3.2-11b-vision-instruct\n",
            "   meta-llama/llama-3.2-1b-instruct\n",
            "   meta-llama/llama-3.2-3b-instruct\n",
            "   meta-llama/llama-3.2-90b-vision-instruct\n",
            "   meta-llama/llama-3.3-70b-instruct\n",
            "   meta-llama/llama-4-maverick\n",
            "   meta-llama/llama-4-scout\n",
            "   meta-llama/llama-guard-3-8b\n",
            "   meta-llama/llama-guard-4-12b\n",
            "   neversleep/llama-3.1-lumimaid-8b\n",
            "   nousresearch/hermes-2-pro-llama-3-8b\n",
            "   nvidia/llama-3.1-nemotron-70b-instruct\n",
            "   nvidia/llama-3.1-nemotron-ultra-253b-v1\n",
            "   nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
            "   qwen/qwen-2.5-72b-instruct\n",
            "   qwen/qwen-2.5-7b-instruct\n",
            "   qwen/qwen-2.5-coder-32b-instruct\n",
            "   qwen/qwen-max\n",
            "   qwen/qwen-plus\n",
            "   qwen/qwen-plus-2025-07-28\n",
            "   qwen/qwen-plus-2025-07-28:thinking\n",
            "   qwen/qwen-turbo\n",
            "   qwen/qwen-vl-max\n",
            "   qwen/qwen2.5-coder-7b-instruct\n",
            "   qwen/qwen2.5-vl-32b-instruct\n",
            "   qwen/qwen2.5-vl-72b-instruct\n",
            "   qwen/qwen3-14b\n",
            "   qwen/qwen3-235b-a22b\n",
            "   qwen/qwen3-235b-a22b-2507\n",
            "   qwen/qwen3-235b-a22b-thinking-2507\n",
            "   qwen/qwen3-30b-a3b\n",
            "   qwen/qwen3-30b-a3b-instruct-2507\n",
            "   qwen/qwen3-30b-a3b-thinking-2507\n",
            "   qwen/qwen3-8b\n",
            "   qwen/qwen3-coder\n",
            "   qwen/qwen3-coder-30b-a3b-instruct\n",
            "   qwen/qwen3-coder-flash\n",
            "   qwen/qwen3-coder-plus\n",
            "   qwen/qwen3-coder:exacto\n",
            "   qwen/qwen3-max\n",
            "   qwen/qwen3-next-80b-a3b-instruct\n",
            "   qwen/qwen3-next-80b-a3b-thinking\n",
            "   qwen/qwen3-vl-235b-a22b-instruct\n",
            "   qwen/qwen3-vl-235b-a22b-thinking\n",
            "   qwen/qwen3-vl-30b-a3b-instruct\n",
            "   qwen/qwen3-vl-30b-a3b-thinking\n",
            "   qwen/qwen3-vl-8b-instruct\n",
            "   qwen/qwen3-vl-8b-thinking\n",
            "   qwen/qwq-32b\n"
          ]
        }
      ],
      "source": [
        "# Run fresh evaluation\n",
        "open_source_models = filter_open_source_models(martian_models_json)\n",
        "print(f\"Found {len(open_source_models)} open source models\")\n",
        "for the_model in open_source_models:\n",
        "  print(\"  \", the_model[\"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "450a3cee",
      "metadata": {
        "id": "450a3cee"
      },
      "outputs": [],
      "source": [
        "def get_test_tasks(num_tasks: int = NUM_TEST_TASKS) -> list[str]:\n",
        "    \"\"\"\n",
        "    Select the first N tasks for testing.\n",
        "\n",
        "    Args:\n",
        "        num_tasks: Number of tasks to return\n",
        "\n",
        "    Returns:\n",
        "        List of task names\n",
        "    \"\"\"\n",
        "    return maths_tasks[:num_tasks]\n",
        "\n",
        "\n",
        "def evaluate_model_on_tasks(\n",
        "    model: dict,\n",
        "    data_df: pd.DataFrame,\n",
        "    tasks: list[str],\n",
        "    num_examples: int = NUM_EXAMPLES_PER_TASK,\n",
        "    verbose: bool = False\n",
        ") -> dict[str, int]:\n",
        "    \"\"\"\n",
        "    Evaluate a model's accuracy on multiple tasks.\n",
        "\n",
        "    For each task, tests the model on a sample of examples. Stops testing a task\n",
        "    on first failure to save time.\n",
        "\n",
        "    Args:\n",
        "        model: Model dictionary with 'id' field\n",
        "        data_df: DataFrame containing task examples with 'task', 'prompt', 'ground_truth' columns\n",
        "        tasks: List of task names to evaluate\n",
        "        num_examples: Number of examples to test per task\n",
        "        verbose: Whether to print detailed results\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping task names to number of correct answers\n",
        "    \"\"\"\n",
        "    model_id = model['id']\n",
        "    task_results = {}\n",
        "\n",
        "    for task in tasks:\n",
        "        # Sample examples for this task\n",
        "        task_examples = data_df[data_df['task'] == task].sample(\n",
        "            n=num_examples,\n",
        "            random_state=RANDOM_SEED\n",
        "        )\n",
        "\n",
        "        correct_count = 0\n",
        "\n",
        "        for _, example in task_examples.iterrows():\n",
        "            try:\n",
        "                answer, is_correct = run_model_inference(\n",
        "                    model_id,\n",
        "                    example['prompt'],\n",
        "                    example['ground_truth']\n",
        "                )\n",
        "            except Exception as e:\n",
        "                answer = f\"Error: {str(e)}\"\n",
        "                is_correct = False\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Model: {model_id}\\nOutput/Error: {answer}\\nSuccess: {is_correct}\\n{'-'*40}\")\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            else:\n",
        "                # Early exit on first failure for this task\n",
        "                break\n",
        "\n",
        "        task_results[task] = correct_count\n",
        "\n",
        "    return task_results\n",
        "\n",
        "\n",
        "def find_high_accuracy_models(\n",
        "    candidate_models: list[dict],\n",
        "    data_df: pd.DataFrame,\n",
        "    tasks: list[str],\n",
        "    num_examples: int = NUM_EXAMPLES_PER_TASK,\n",
        "    max_models: int = MAX_MODELS_TO_FIND,\n",
        "    required_accuracy: int = None,\n",
        "    verbose: bool = False\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Find models that achieve perfect or near-perfect accuracy on all tasks.\n",
        "\n",
        "    Args:\n",
        "        candidate_models: List of model dictionaries to evaluate\n",
        "        data_df: DataFrame containing task examples\n",
        "        tasks: List of task names to evaluate\n",
        "        num_examples: Number of examples to test per task\n",
        "        max_models: Maximum number of good models to find\n",
        "        required_accuracy: Required correct answers per task (defaults to num_examples)\n",
        "\n",
        "    Returns:\n",
        "        List of model IDs that meet the accuracy threshold\n",
        "    \"\"\"\n",
        "    if required_accuracy is None:\n",
        "        required_accuracy = num_examples\n",
        "\n",
        "    print(f\"Some models with perfect accuracy on {NUM_EXAMPLES_PER_TASK} instances of first {NUM_TEST_TASKS} tasks:\")\n",
        "    high_accuracy_models = []\n",
        "\n",
        "    for model in candidate_models:\n",
        "        task_scores = evaluate_model_on_tasks(model, data_df, tasks, num_examples, verbose)\n",
        "\n",
        "        # Check if model achieved required accuracy on all tasks\n",
        "        if all(score >= required_accuracy for score in task_scores.values()):\n",
        "            high_accuracy_models.append(model['id'])\n",
        "            print( \"  Good:\", model['id'] )\n",
        "        elif verbose:\n",
        "            print(f\"Model: {model['id']}, Task Scores: {task_scores}\")\n",
        "\n",
        "        # Stop if we've found enough models\n",
        "        if len(high_accuracy_models) >= max_models:\n",
        "            break\n",
        "\n",
        "    return high_accuracy_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0e7db601",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0e7db601",
        "outputId": "334a97e2-6a89-4c90-d594-85979fc0bb3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some models with perfect accuracy on 5 instances of first 4 tasks:\n",
            "Model: aion-labs/aion-rp-llama-3.1-8b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '497e588f-1286-4c45-b1b0-b33ea6347e9d'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: aion-labs/aion-rp-llama-3.1-8b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'd3675296-07fb-418c-9136-6edc2f61670a'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: aion-labs/aion-rp-llama-3.1-8b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'b3c2db9e-3671-441e-bb52-f5c4042a6185'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: aion-labs/aion-rp-llama-3.1-8b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '489fc591-7576-4b80-b12c-dfbb695cb233'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: aion-labs/aion-rp-llama-3.1-8b, Task Scores: {'minimum': 0, 'maximum': 0, 'sum': 0, 'difference': 0}\n",
            "Model: deepcogito/cogito-v2-preview-llama-405b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '39cc3d59-1871-4e50-a545-3669fd9dc683'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepcogito/cogito-v2-preview-llama-405b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '936848f7-23a0-4bbb-8eeb-3e1fa4140178'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepcogito/cogito-v2-preview-llama-405b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '1d029698-78bd-4cbf-9899-7adc26d46e0a'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepcogito/cogito-v2-preview-llama-405b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'b19e73c9-3bdb-4582-8dcd-a01fb9ac485e'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepcogito/cogito-v2-preview-llama-405b, Task Scores: {'minimum': 0, 'maximum': 0, 'sum': 0, 'difference': 0}\n",
            "Model: deepcogito/cogito-v2-preview-llama-70b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '560e7a7a-c08e-423f-b5b9-de2564d55ecf'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepcogito/cogito-v2-preview-llama-70b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'e8e1e17d-4aed-4c23-811e-985c58565aed'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepcogito/cogito-v2-preview-llama-70b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '03744b5c-4269-4f35-b6e9-35615a7293f8'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepcogito/cogito-v2-preview-llama-70b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '719b05d9-b583-42ed-b0e8-035cf0837096'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepcogito/cogito-v2-preview-llama-70b, Task Scores: {'minimum': 0, 'maximum': 0, 'sum': 0, 'difference': 0}\n",
            "Model: deepinfra/openai/gpt-oss-120b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'ece822d4-eb1b-4528-8921-dc55cf65fde3'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepinfra/openai/gpt-oss-120b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'fbe50ada-7e07-4cc2-835d-a84d474d422d'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepinfra/openai/gpt-oss-120b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'e4b1b4c0-348b-4a62-9988-332535ea2b30'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepinfra/openai/gpt-oss-120b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '8a2431cf-5786-4652-9a1f-531340d6baf1'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepinfra/openai/gpt-oss-120b, Task Scores: {'minimum': 0, 'maximum': 0, 'sum': 0, 'difference': 0}\n",
            "Model: deepinfra/openai/gpt-oss-20b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '65945838-1560-4a48-880e-8bd003317dd8'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepinfra/openai/gpt-oss-20b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'aadc7387-1db3-42ee-ba40-9e9501b74be9'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepinfra/openai/gpt-oss-20b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'adf2ad45-e235-4481-a3c9-4e055375d2e8'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepinfra/openai/gpt-oss-20b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '0001ff91-564c-401f-bf41-916b901475ee'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepinfra/openai/gpt-oss-20b, Task Scores: {'minimum': 0, 'maximum': 0, 'sum': 0, 'difference': 0}\n",
            "Model: deepseek/deepseek-r1-0528-qwen3-8b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '43a46fd1-013f-453d-ba1c-f0cfd95403de'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepseek/deepseek-r1-0528-qwen3-8b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '76bebe4a-3c7c-4f88-9c1e-71b34d582b94'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepseek/deepseek-r1-0528-qwen3-8b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '654bdf2e-211e-4d1f-8f8a-4c52e34a195b'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepseek/deepseek-r1-0528-qwen3-8b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '9c2a79a7-15e7-4c01-974c-16a6ff69b796'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepseek/deepseek-r1-0528-qwen3-8b, Task Scores: {'minimum': 0, 'maximum': 0, 'sum': 0, 'difference': 0}\n",
            "Model: deepseek/deepseek-r1-distill-qwen-32b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'a9497d27-064b-497d-8305-a2d52de52c4c'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepseek/deepseek-r1-distill-qwen-32b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '415208ff-4787-4a5f-b896-0c9ba3045729'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepseek/deepseek-r1-distill-qwen-32b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'b28acaa5-ea12-456e-8b52-ee2a3b0a8ab8'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepseek/deepseek-r1-distill-qwen-32b\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '800be207-5c0e-4e34-a1f7-bbeec5eafb52'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: deepseek/deepseek-r1-distill-qwen-32b, Task Scores: {'minimum': 0, 'maximum': 0, 'sum': 0, 'difference': 0}\n",
            "Model: meta-llama/llama-3-70b-instruct\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'c33a337a-51c8-4e76-9aea-bb69e0b213fe'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: meta-llama/llama-3-70b-instruct\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': 'af773197-27b9-4d0c-931f-2342c27bf079'}\n",
            "Success: False\n",
            "----------------------------------------\n",
            "Model: meta-llama/llama-3-70b-instruct\n",
            "Output/Error: Error: Error code: 402 - {'error': 'Insufficient balance', 'request_id': '235ff10e-1fba-4525-a66a-d2920a604476'}\n",
            "Success: False\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1394507335.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TEST_TASKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m good_models = find_high_accuracy_models(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mopen_source_models\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msynthetic_data_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_tasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-449659138.py\u001b[0m in \u001b[0;36mfind_high_accuracy_models\u001b[0;34m(candidate_models, data_df, tasks, num_examples, max_models, required_accuracy, verbose)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidate_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mtask_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_on_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Check if model achieved required accuracy on all tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-449659138.py\u001b[0m in \u001b[0;36mevaluate_model_on_tasks\u001b[0;34m(model, data_df, tasks, num_examples, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtask_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 answer, is_correct = run_model_inference(\n\u001b[0m\u001b[1;32m     52\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prompt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3156718239.py\u001b[0m in \u001b[0;36mrun_model_inference\u001b[0;34m(model_name, prompt, ground_truth, timeout)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# The OpenAI client has its own timeout parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    983\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "test_tasks = get_test_tasks(NUM_TEST_TASKS)\n",
        "good_models = find_high_accuracy_models(\n",
        "    open_source_models,\n",
        "    synthetic_data_df,\n",
        "    test_tasks,\n",
        "    num_examples=NUM_EXAMPLES_PER_TASK,\n",
        "    max_models=MAX_MODELS_TO_FIND,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "07df8fc3",
      "metadata": {
        "id": "07df8fc3"
      },
      "outputs": [],
      "source": [
        "if not IN_COLAB:\n",
        "    # Persist good models to file\n",
        "    output_file = os.path.join(DATA_DIR, f\"GoodOpenModels_{NUM_TEST_TASKS}Tasks_{NUM_EXAMPLES_PER_TASK}Tests.json\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(good_models, f, indent=2)\n",
        "    print(f\"Saved good models to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8568249",
      "metadata": {
        "id": "f8568249"
      },
      "source": [
        "## Manually inspect model output\n",
        "\n",
        "We want to avoid models that use a python sandbox to do math. This is sometimes visible in the answer detail. View sample answers here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7e02c545",
      "metadata": {
        "id": "7e02c545"
      },
      "outputs": [],
      "source": [
        "# For each of the top models, ask one instance of one task and show the answer\n",
        "def inspect_model_answers():\n",
        "    for model_name in good_models:\n",
        "        task = random.choice(maths_tasks)\n",
        "        example_df = synthetic_data_df[synthetic_data_df['task'] == task].sample(n=1, random_state=42).iloc[0]\n",
        "        print(f\"\\nModel: {model_name}\\nTask: {task}\\nPrompt: {example_df['prompt']}\\nGround Truth: {example_df['ground_truth']}\")\n",
        "        answer, success = run_model_inference(model_name, example_df['prompt'], example_df['ground_truth'])\n",
        "        print(f\"Answer: {answer}\\nSuccess: {success}\\n{'-'*60}\")\n",
        "\n",
        "# inspect_model_answers()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}